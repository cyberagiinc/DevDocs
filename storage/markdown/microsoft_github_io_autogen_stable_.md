# Skip to main content
URL: https://microsoft.github.io/autogen/stable/#main-content

###  A framework for building AI agents and applications

A console-based multi-agent assistant for web and file-based tasks. Built on
AgentChat.

```

pipinstall-Umagentic-one-cli

m1"Find flights from Seattle to Paris and format the result in a table"

```

An app for prototyping and managing agents without writing code. Built on
AgentChat.

A programming framework for building conversational single and multi-agent
applications. Built on Core. Requires Python 3.10+.

```

# pip install -U "autogen-agentchat" "autogen-ext[openai]"



  
  
    
     
  

```

_Start here if you are building conversational agents. ._

An event-driven programming framework for building scalable multi-agent AI
systems. Example scenarios:

  * Deterministic and dynamic agentic workflows for business processes.
  * Distributed agents for multi-language applications.

_Start here if you are building workflows or distributed agent systems._

Implementations of Core and AgentChat components that interface with external
services or other libraries. You can find and use community extensions or create
your own. Examples of built-in extensions:

  * for running model-generated code in a Docker container.

---

# Switch to stable version
URL: https://microsoft.github.io/autogen/stable/index.html

###  A framework for building AI agents and applications

A console-based multi-agent assistant for web and file-based tasks. Built on
AgentChat.

```

pipinstall-Umagentic-one-cli

m1"Find flights from Seattle to Paris and format the result in a table"

```

An app for prototyping and managing agents without writing code. Built on
AgentChat.

A programming framework for building conversational single and multi-agent
applications. Built on Core. Requires Python 3.10+.

```

# pip install -U "autogen-agentchat" "autogen-ext[openai]"



  
  
    
     
  

```

_Start here if you are building conversational agents. ._

An event-driven programming framework for building scalable multi-agent AI
systems. Example scenarios:

  * Deterministic and dynamic agentic workflows for business processes.
  * Distributed agents for multi-language applications.

_Start here if you are building workflows or distributed agent systems._

Implementations of Core and AgentChat components that interface with external
services or other libraries. You can find and use community extensions or create
your own. Examples of built-in extensions:

  * for running model-generated code in a Docker container.

---

# Untitled Page
URL: https://microsoft.github.io/autogen/stable/

###  A framework for building AI agents and applications

A console-based multi-agent assistant for web and file-based tasks. Built on
AgentChat.

```

pipinstall-Umagentic-one-cli

m1"Find flights from Seattle to Paris and format the result in a table"

```

An app for prototyping and managing agents without writing code. Built on
AgentChat.

A programming framework for building conversational single and multi-agent
applications. Built on Core. Requires Python 3.10+.

```

# pip install -U "autogen-agentchat" "autogen-ext[openai]"



  
  
    
     
  

```

_Start here if you are building conversational agents. ._

An event-driven programming framework for building scalable multi-agent AI
systems. Example scenarios:

  * Deterministic and dynamic agentic workflows for business processes.
  * Distributed agents for multi-language applications.

_Start here if you are building workflows or distributed agent systems._

Implementations of Core and AgentChat components that interface with external
services or other libraries. You can find and use community extensions or create
your own. Examples of built-in extensions:

  * for running model-generated code in a Docker container.

---

# dev (main)
URL: https://microsoft.github.io/autogen/dev/index.html

###  A framework for building AI agents and applications

A console-based multi-agent assistant for web and file-based tasks. Built on
AgentChat.

```

pipinstall-Umagentic-one-cli

m1"Find flights from Seattle to Paris and format the result in a table"

```

An app for prototyping and managing agents without writing code. Built on
AgentChat.

A programming framework for building conversational single and multi-agent
applications. Built on Core. Requires Python 3.10+.

```

# pip install -U "autogen-agentchat" "autogen-ext[openai]"





  
     
  

```

_Start here if you are building conversational agents. ._

An event-driven programming framework for building scalable multi-agent AI
systems. Example scenarios:

  * Deterministic and dynamic agentic workflows for business processes.
  * Distributed agents for multi-language applications.

_Start here if you are building workflows or distributed agent systems._

Implementations of Core and AgentChat components that interface with external
services or other libraries. You can find and use community extensions or create
your own. Examples of built-in extensions:

  * for running model-generated code in a Docker container.

---

# 0.2
URL: https://microsoft.github.io/autogen/0.2/index.html

has been released. Read the migration guide .

AutoGen provides multi-agent conversation framework as a high-level abstraction.
With this framework, one can conveniently build LLM workflows.

AutoGen offers a collection of working systems spanning a wide range of
applications from various domains and complexities.

### [Enhanced LLM Inference &
Optimization](https://microsoft.github.io/autogen/0.2/</autogen/0.2/docs/Use-
Cases/enhanced_inference>)

AutoGen supports enhanced LLM inference APIs, which can be used to improve
inference performance and reduce cost.

has been released. Read the migration guide .

AutoGen provides multi-agent conversation framework as a high-level abstraction.
With this framework, one can conveniently build LLM workflows.

AutoGen offers a collection of working systems spanning a wide range of
applications from various domains and complexities.

### [Enhanced LLM Inference &
Optimization](https://microsoft.github.io/autogen/0.2/</autogen/docs/Use-
Cases/enhanced_inference>)

AutoGen supports enhanced LLM inference APIs, which can be used to improve
inference performance and reduce cost.

---

# AgentChat
URL: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html

AgentChat is a high-level API for building multi-agent applications. It is built
on top of the package. For beginner users, AgentChat is the recommended starting
point. For advanced users, ’s event-driven programming model provides more
flexibility and control over the underlying components.

AgentChat provides intuitive defaults, such as with preset behaviors and with
predefined .

Step-by-step guide to using AgentChat, learn about agents, teams, and more

Multi-agent coordination through a shared context and centralized, customizable
selector

Multi-agent coordination through a shared context and localized, tool-based
selector

Sample code and use cases

How to migrate from AutoGen 0.2.x to 0.4.x.

---

# Core
URL: https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html

AutoGen core offers an easy way to quickly build event-driven, distributed,
scalable, resilient AI agent systems. Agents are developed by using the . You
can build and run your agent system locally and easily move to a distributed
system in the cloud when you are ready.

Key features of AutoGen core include:

Agents communicate through asynchronous messages, enabling event-driven and
request/response communication models.

Enable complex scenarios with networks of agents across organizational
boundaries.

Python & Dotnet interoperating agents today, with more languages coming soon.

Highly customizable with features like custom agents, memory as a service, tools
registry, and model library.

Easily trace and debug your agent systems.

Build event-driven, distributed, scalable, and resilient AI agent systems.

---

# Extensions
URL: https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html

AutoGen is designed to be extensible. The package contains many different
component implementations maintained by the AutoGen project. However, we
strongly encourage others to build their own components and publish them as part
of the ecosytem.

Discover community extensions and samples

---

# Studio
URL: https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html

AutoGen Studio is a low-code interface built to help you rapidly prototype AI
agents, enhance them with tools, compose them into teams and interact with them
to accomplish tasks. It is built on - a high-level API for building multi-agent
applications.

Code for AutoGen Studio is on GitHub at

AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and
demonstrate an example of end user interfaces built with AutoGen. It is not
meant to be a production-ready app. Developers are encouraged to use the AutoGen
framework to build their own applications, implementing authentication, security
and other features required for deployed applications.

## Capabilities - What Can You Do with AutoGen Studio?

AutoGen Studio offers four main interfaces to help you build and manage multi-
agent systems:

  1.      * A visual interface for creating agent teams through declarative specification (JSON) or drag-and-drop
     * Supports configuration of all core components: teams, agents, tools, models, and termination conditions
     * Fully compatible with AgentChat’s component definitions
  2.      * Interactive environment for testing and running agent teams
     *        * Live message streaming between agents
       * Visual representation of message flow through a control transition graph
       * Interactive sessions with teams using UserProxyAgent
       * Full run control with the ability to pause or stop execution
  3.      * Central hub for discovering and importing community-created components
     * Enables easy integration of third-party components
  4.      * Export and run teams in python code
     * Setup and test endpoints based on a team configuration
     * Run teams in a docker container

Review project roadmap and issues .

We welcome contributions to AutoGen Studio. We recommend the following general
steps to contribute to the project:

  * Review the overall AutoGen project 
  * Please review the AutoGen Studio to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with 
  * Please use the tag tag for any issues, questions, and PRs related to Studio
  * Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.
  * Submit a pull request with your contribution!
  * If you are modifying AutoGen Studio, it has its own devcontainer. See instructions in to use it

AutoGen Studio is a research prototype and is **not meant to be used** in a
production environment. Some baseline practices are encouraged e.g., using
Docker code execution environment for your agents.

However, other considerations such as rigorous tests related to jailbreaking,
ensuring LLMs only have access to the right keys of data given the end user’s
permissions, and other security features are not implemented in AutoGen Studio.

If you are building a production application, please use the AutoGen framework
and implement the necessary security features.

AutoGen Studio is based on the project. It was adapted from a research prototype
built in October 2023 (original credits: Victor Dibia, Gagan Bansal, Adam
Fourney, Piali Choudhury, Saleema Amershi, Ahmed Awadallah, Chi Wang).

If you use AutoGen Studio in your research, please cite the following paper:

To begin, follow the to install AutoGen Studio.

---

# 0.2 Docs
URL: https://microsoft.github.io/autogen/0.2/

has been released. Read the migration guide .

AutoGen provides multi-agent conversation framework as a high-level abstraction.
With this framework, one can conveniently build LLM workflows.

AutoGen offers a collection of working systems spanning a wide range of
applications from various domains and complexities.

### [Enhanced LLM Inference &
Optimization](https://microsoft.github.io/autogen/0.2/</autogen/0.2/docs/Use-
Cases/enhanced_inference>)

AutoGen supports enhanced LLM inference APIs, which can be used to improve
inference performance and reduce cost.

---

# #
URL: https://microsoft.github.io/autogen/stable/#autogen

###  A framework for building AI agents and applications

A console-based multi-agent assistant for web and file-based tasks. Built on
AgentChat.

```

pipinstall-Umagentic-one-cli

m1"Find flights from Seattle to Paris and format the result in a table"

```

An app for prototyping and managing agents without writing code. Built on
AgentChat.

A programming framework for building conversational single and multi-agent
applications. Built on Core. Requires Python 3.10+.

```

# pip install -U "autogen-agentchat" "autogen-ext[openai]"



  
  
    
     
  

```

_Start here if you are building conversational agents. ._

An event-driven programming framework for building scalable multi-agent AI
systems. Example scenarios:

  * Deterministic and dynamic agentic workflows for business processes.
  * Distributed agents for multi-language applications.

_Start here if you are building workflows or distributed agent systems._

Implementations of Core and AgentChat components that interface with external
services or other libraries. You can find and use community extensions or create
your own. Examples of built-in extensions:

  * for running model-generated code in a Docker container.

---

# Get Started
URL: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html

is a generalist multi-agent system for solving open-ended web and file-based
tasks across a variety of domains. It represents a significant step forward for
multi-agent systems, achieving competitive performance on a number of agentic
benchmarks (see the for full details).

When originally released in Magentic-One was . We have now ported Magentic-One
to use , providing a more modular and easier to use interface.

To this end, the Magentic-One orchestrator is now simply an AgentChat team,
supporting all standard AgentChat agents and features. Likewise, Magentic-One’s
, , and agents are now broadly available as AgentChat agents, to be used in any
AgentChat workflows.

Lastly, there is a helper class, , which bundles all of this together as it was
in the paper with minimal configuration.

Find additional information about Magentic-one in our and .

: The figure above illustrates Magentic-One multi-agent team completing a
complex task from the GAIA benchmark. Magentic-One’s Orchestrator agent creates
a plan, delegates tasks to other agents, and tracks progress towards the goal,
dynamically revising the plan as needed. The Orchestrator can delegate tasks to
a FileSurfer agent to read and handle files, a WebSurfer agent to operate a web
browser, or a Coder or Computer Terminal agent to write or execute code,
respectively.

Using Magentic-One involves interacting with a digital world designed for
humans, which carries inherent risks. To minimize these risks, consider the
following precautions:

  1. : Run all tasks in docker containers to isolate the agents and prevent direct system attacks.
  2. : Use a virtual environment to run the agents and prevent them from accessing sensitive data.
  3. : Closely monitor logs during and after execution to detect and mitigate risky behavior.
  4. : Run the examples with a human in the loop to supervise the agents and prevent unintended consequences.
  5. : Restrict the agents’ access to the internet and other resources to prevent unauthorized actions.
  6. : Ensure that the agents do not have access to sensitive data or resources that could be compromised. Do not share sensitive information with the agents. Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences. Moreover, be cautious that Magentic-One may be susceptible to prompt injection attacks from webpages.

```

pipinstallautogen-agentchatautogen-extmagentic-one,openai

# If using the MultimodalWebSurfer, you also need to install playwright
dependencies:

playwrightinstall--with-depschromium

```

If you haven’t done so already, go through the AgentChat tutorial to learn about
the concepts of AgentChat.

Then, you can try swapping out a with . For example:

```



  
  
  
  

    
    
    
    
    
  
     
   "Provide a different proof for Fermat's Last Theorem"

```

Or, use the Magentic-One agents in a team:

The example code may download files from the internet, execute code, and
interact with web pages. Ensure you are in a safe environment before running the
example code.

```



  
  
  
  

    
    
    
    
    
  
     
   "What is the UV index in Melbourne today?"

```

Magentic-One work is based on a multi-agent architecture where a lead
Orchestrator agent is responsible for high-level planning, directing other
agents and tracking task progress. The Orchestrator begins by creating a plan to
tackle the task, gathering needed facts and educated guesses in a Task Ledger
that is maintained. At each step of its plan, the Orchestrator creates a
Progress Ledger where it self-reflects on task progress and checks whether the
task is completed. If the task is not yet completed, it assigns one of Magentic-
One other agents a subtask to complete. After the assigned agent completes its
subtask, the Orchestrator updates the Progress Ledger and continues in this way
until the task is complete. If the Orchestrator finds that progress is not being
made for enough steps, it can update the Task Ledger and create a new plan. This
is illustrated in the figure above; the Orchestrator work is thus divided into
an outer loop where it updates the Task Ledger and an inner loop to update the
Progress Ledger.

Overall, Magentic-One consists of the following agents:

  * Orchestrator: the lead agent responsible for task decomposition and planning, directing other agents in executing subtasks, tracking overall progress, and taking corrective actions as needed
  * WebSurfer: This is an LLM-based agent that is proficient in commanding and managing the state of a Chromium-based web browser. With each incoming request, the WebSurfer performs an action on the browser then reports on the new state of the web page The action space of the WebSurfer includes navigation (e.g. visiting a URL, performing a web search); web page actions (e.g., clicking and typing); and reading actions (e.g., summarizing or answering questions). The WebSurfer relies on the accessibility tree of the browser and on set-of-marks prompting to perform its actions.
  * FileSurfer: This is an LLM-based agent that commands a markdown-based file preview application to read local files of most types. The FileSurfer can also perform common navigation tasks such as listing the contents of directories and navigating a folder structure.
  * Coder: This is an LLM-based agent specialized through its system prompt for writing code, analyzing information collected from the other agents, or creating new artifacts.
  * ComputerTerminal: Finally, ComputerTerminal provides the team with access to a console shell where the Coder’s programs can be executed, and where new programming libraries can be installed.

Together, Magentic-One’s agents provide the Orchestrator with the tools and
capabilities that it needs to solve a broad variety of open-ended problems, as
well as the ability to autonomously adapt to, and act in, dynamic and ever-
changing web and file-system environments.

While the default multimodal LLM we use for all agents is GPT-4o, Magentic-One
is model agnostic and can incorporate heterogonous models to support different
capabilities or meet different cost requirements when getting tasks done. For
example, it can use different LLMs and SLMs and their specialized versions to
power different agents. We recommend a strong reasoning model for the
Orchestrator agent such as GPT-4o. In a different configuration of Magentic-One,
we also experiment with using OpenAI o1-preview for the outer loop of the
Orchestrator and for the Coder, while other agents continue to use GPT-4o.

---

# Migrating from AutoGen 0.2?
URL: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html

# Migration Guide for v0.2 to v0.4

This is a migration guide for users of the versions of to the version, which
introduces a new set of APIs and features. The version contains breaking
changes. Please read this guide carefully. We still maintain the version in the
branch; however, we highly recommend you upgrade to the version.

We no longer have admin access to the PyPI package, and the releases from that
package are no longer from Microsoft since version 0.2.34. To continue use the
version of AutoGen, install it using . Please read our regarding forks.

Since the release of AutoGen in 2023, we have intensively listened to our
community and users from small startups and large enterprises, gathering much
feedback. Based on that feedback, we built AutoGen , a from-the-ground-up
rewrite adopting an asynchronous, event-driven architecture to address issues
such as observability, flexibility, interactive control, and scale.

The API is layered: the is the foundation layer offering a scalable, event-
driven actor framework for creating agentic workflows; the is built on Core,
offering a task-driven, high-level framework for building interactive agentic
applications. It is a replacement for AutoGen .

Most of this guide focuses on ’s AgentChat API; however, you can also build your
own high-level framework using just the Core API.

Jump straight to the to get started with .

We provide a detailed guide on how to migrate your existing codebase from to .

See each feature below for detailed information on how to migrate.

  * [Model Client for OpenAI-Compatible APIs](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#model-client-for-openai-compatible-apis>)
  * [Conversable Agent and Register Reply](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#conversable-agent-and-register-reply>)
  * [Save and Load Agent State](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#save-and-load-agent-state>)
  * [Conversion between v0.2 and v0.4 Messages](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#conversion-between-v0-2-and-v0-4-messages>)
  * [Save and Load Group Chat State](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#save-and-load-group-chat-state>)
  * [Group Chat with Tool Use](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#group-chat-with-tool-use>)
  * [Group Chat with Custom Selector (Stateflow)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#group-chat-with-custom-selector-stateflow>)

The following features currently in will be provided in the future releases of
versions:

We will update this guide when the missing features become available.

In you configure the model client as follows, and create the object.

> : In AutoGen 0.2, the OpenAI client would try configs in the list until one
> worked. 0.4 instead expects a specfic model configuration to be chosen.
In , we offer two ways to create a model client.

AutoGen 0.4 has a . Model clients are a great use case for this. See below for
how to create an OpenAI chat completion client.

### Use model client class directly

## Model Client for OpenAI-Compatible APIs

You can use a the to connect to an OpenAI-Compatible API, but you need to
specify the and .

> : We don’t test all the OpenAI-Compatible APIs, and many of them works
> differently from the OpenAI API even though they may claim to suppor it.
> Please test them before using them.
Read about in AgentChat Tutorial and more detailed information on

Support for other hosted models will be added in the future.

In , you create an assistant agent as follows:

```

  
  
      
  
  

  
  
  "You are a helpful assistant."

  

```

In , it is similar, but you need to specify instead of .

```

  
  
     
  
  
  "You are a helpful assistant."

  

```

However, the usage is somewhat different. In , instead of calling , you call or
to handle incoming messages. Furthermore, the and methods are asynchronous, and
the latter returns an async generator to stream the inner thoughts of the agent.

Here is how you can call the assistant agent in directly, continuing from the
above example:

```



  
  
  
  
    
      
    
    
    "You are a helpful assistant."
    
  
    
       
  

```

The can be used to cancel the request asynchronously when you call , which will
cause the on the call to raise a .

The in supports multi-modal inputs if the model client supports it. The
capability of the model client is used to determine if the agent supports multi-
modal inputs.

```



  
  
  
    
  
    
      
    
    
    "You are a helpful assistant."
    
  
    
    
     
    
  
      
  

```

In , you create a user proxy as follows:

This user proxy would take input from the user through console, and would
terminate if the incoming message ends with “TERMINATE”.

In , a user proxy is simply an agent that takes user input only, there is no
other special configuration needed. You can create a user proxy as follows:

See for more details and how to customize the input function with timeout.

## Conversable Agent and Register Reply

In , you can create a conversable agent and register a reply function as
follows:

```

        
  
  
      
  
  

  
  
  "You are a helpful assistant."

  
  
  
  



  
     
     
     
     
  # Custom reply logic here

    
# Register the reply function

  
# NOTE: An async reply function will only be invoked with async send.

```

Rather than guessing what the does, all its parameters, and what the should be,
in , we can simply create a custom agent and implement the , , and methods.

You can then use the custom agent in the same way as the . See for more details.

## Save and Load Agent State

In there is no built-in way to save and load an agent’s state: you need to
implement it yourself by exporting the attribute of and importing it back
through the parameter.

In , you can call and methods on agents to save and load their state.

```





  
  
  
  
    
      
    
    
    "You are a helpful assistant."
    
  
    
       
  
  
     
  # (Optional) Write state to disk.

      
     
  # (Optional) Load it back from disk.

      
      
     # Inspect the state, which contains the chat history.
  # Carry on the chat.

       
  
  # Load the state, resulting the agent to revert to the previous state before
the last message.

  
  # Carry on the same chat again.

       

```

You can also call and on any teams, such as to save and load the state of the
entire team.

In , you can create a two-agent chat for code execution as follows:

```

  
    
  
      
  
  

  
  
  "You are a helpful assistant. Write all code in python. Reply only 'TERMINATE'
if the task is done."

  
     

  
  
  
  
  
  
     

   "Write a python script to print 'Hello, world!'"

# Intermediate messages are printed to the console directly.

```

To get the same behavior in , you can use the and together in a .

```



    
  
    
  
  
  
    
      
    
    
    "You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done."
    
  
    
    
    
  
  # The termination condition is a combination of text termination and max
message termination, either of which will cause the chat to terminate.

      
  # The group chat will alternate between the assistant and the code executor.

      
  # `run_stream` returns an async generator to stream the intermediate messages.

    "Write a python script to print 'Hello, world!'"
  # `Console` is a simple UI to display the stream.

  

```

In , to create a tool use chatbot, you must have two agents, one for calling the
tool and one for executing the tool. You need to initiate a two-agent chat for
every user request.

```

     
  
      
  
  

  
  
  "You are a helpful assistant. You can call tools to help user."

  
   # Set to 1 so that we return to the application after each assistant reply as
we are building a chatbot.

  
  
  
  
  

    
    is 72 degree and sunny."
# Register the tool function to the tool caller and executor.

  


    
     
    
    
    
    
     # To let the model reflect on the tool use, set to "last_msg" to return the tool call result directly.
  
  

```

In , you really just need one agent – the – to handle both the tool calling and
tool execution.

```



  
  
  
  
     # Async tool is possible too.
    is 72 degree and sunny."
    
      
    
    
    "You are a helpful assistant. You can call tools to help user."
    
    
     # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly.
  
  
      
       
      
         
     

```

When using tool-equipped agents inside a group chat such as , you simply do the
same as above to add tools to the agents, and create a group chat with the
agents.

In , you get a object from the method. For example:

```

  
  
  
  

 # Get LLM-reflected summary of the chat.

 # Get the chat history.

 # Get the cost of the chat.

 # Get the human input solicited by the chat.

```

In , you get a object from a or method. The object contains the which is the
message history of the chat, including both agents’ private (tool calls, etc.)
and public messages.

There are some notable differences between and :

  * The list in uses different message format than the list.
  * There is no field. It is up to the application to decide how to summarize the chat using the list.
  * is not provided in the object, as the user input can be extracted from the list by filtering with the field.
  * is not provided in the object, however, you can calculate the cost based on token usage. It would be a great community extension to add cost calculation. See .

## Conversion between v0.2 and v0.4 Messages

You can use the following conversion functions to convert between a v0.4 message
in and a v0.2 message in .

```

      
  
  
  
  
  
  
  
  
  
  

    
  



     
     
       
  
"""Convert a v0.4 AgentChat message to a v0.2 message.

    message (AgentEvent | ChatMessage): The message to convert.
    role (Literal["assistant", "user", "tool"]): The role of the message.
    image_detail (Literal["auto", "high", "low"], optional): The detail level of image content in multi-modal message. Defaults to "auto".

    Dict[str, Any]: The converted AutoGen v0.2 message.

      
          
           
    
           
       
        
           
        
        
      
         
    
             
       
      
        
           
           
              
        
      
    
        
       
      
        
           
           
           
        
      
          
           
  
     
  

       
"""Convert a v0.2 message to a v0.4 AgentChat message."""

     
       
       
      
        
          
          
          
        
      
      
     
       
       
      
        
          
          
        
      
      
    
         
        
          
         
      
         
      
    
      
  
     

```

In , you need to create a class and pass it into a , and have a participant that
is a user proxy to initiate the chat. For a simple scenario of a writer and a
critic, you can do the following:

```

     
  
      
  
  

  
  
  
  
  
     

  
  
  
  "You are a critic, provide feedback on the writing. Reply only 'APPROVE' if
the task is done."

  

# Create a group chat with the writer and critic.

     
# Create a group chat manager to manage the group chat, use round-robin
selection method.

    
# Initiate the chat with the editor, intermediate messages are printed to the
console directly.

  
  
  "Write a short story about a robot that discovers it has feelings."

```

In , you can use the to achieve the same behavior.

```



  
  
  
  
  
    
      
    
    
    
    
    
  
    
    
    
    "You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done."
    
  
  # The termination condition is a text termination, which will cause the chat
to terminate when the text "APPROVE" is received.

    
  # The group chat will alternate between the writer and the critic.

       
  # `run_stream` returns an async generator to stream the intermediate messages.

    "Write a short story about a robot that discovers it has feelings."
  # `Console` is a simple UI to display the stream.

  

```

For LLM-based speaker selection, you can use the instead. See and for more
details.

> : In , you do not need to register functions on a user proxy to use tools in a
> group chat. You can simply pass the tool functions to the as shown in the
> section. The agent will automatically call the tools when needed. If your tool
> doesn’t output well formed response, you can use the parameter to have the
> model reflect on the tool use.
In , group chat with resume is a bit complicated. You need to explicitly save
the group chat messages and load them back when you want to resume the chat. See
[Resuming Group Chat in v0.2](https://microsoft.github.io/autogen/stable/user-
guide/agentchat-user-
guide/<https:/microsoft.github.io/autogen/0.2/docs/topics/groupchat/resuming_groupchat>)
for more details.

In , you can simply call or again with the same group chat object to resume the
chat. To export and load the state, you can use and methods.

```





  
  
  
  
  
  
      
    
    
    
    
    
  
    
    
    
    "You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done."
    
  
  # The termination condition is a text termination, which will cause the chat
to terminate when the text "APPROVE" is received.

    
  # The group chat will alternate between the writer and the critic.

      
  

    
  
    
  # `run_stream` returns an async generator to stream the intermediate messages.

    "Write a short story about a robot that discovers it has feelings."
  # `Console` is a simple UI to display the stream.

  
  # Save the state of the group chat and all participants.

     
      
     
  # Create a new team with the same participants configuration.

    
  # Load the state of the group chat and all participants.

      
      
  
  
    "Translate the story into Chinese."
  

```

## Save and Load Group Chat State

In , you need to explicitly save the group chat messages and load them back when
you want to resume the chat.

In , you can simply call and methods on the group chat object. See for an
example.

## Group Chat with Tool Use

In group chat, when tools are involved, you need to register the tool functions
on a user proxy, and include the user proxy in the group chat. The tool calls
made by other agents will be routed to the user proxy to execute.

We have observed numerous issues with this approach, such as the the tool call
routing not working as expected, and the tool call request and result cannot be
accepted by models without support for function calling.

In , there is no need to register the tool functions on a user proxy, as the
tools are directly executed within the , which publishes the response from the
tool to the group chat. So the group chat manager does not need to be involved
in routing tool calls.

See for an example of using tools in a group chat.

## Group Chat with Custom Selector (Stateflow)

In group chat, when the is set to a custom function, it can override the default
selection method. This is useful for implementing a state-based selection
method. For more details, see [Custom Sepaker Selection in
v0.2](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-
guide/<https:/microsoft.github.io/autogen/0.2/docs/topics/groupchat/customized_speaker_selection>).

In , you can use the with to achieve the same behavior. The is a function that
takes the current message thread of the group chat and returns the next
speaker’s name. If is returned, the LLM-based selection method will be used.

Here is an example of using the state-based selection method to implement a web
search/analysis scenario.

```



  
  
    
    
  
  
  
# Note: This example uses mock tools instead of real APIs for demonstration
purposes

    
     
     """Here are the total points scored by Miami Heat players in the 2006-2007 season:

     
     "The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214."
     
     "The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398."
  

      
         
  
    
    
    
    "An agent for planning tasks, this agent should be the first to engage when given a new task."
    
    
    You are a planning agent.
    Your job is to break down complex tasks into smaller, manageable subtasks.

      Web search agent: Searches for information

    You only plan and delegate tasks - you do not execute them yourself.
    When assigning tasks, use this format:

    After all tasks are complete, summarize the findings and end with "TERMINATE".

  
    
    
    
    
    
    
    You are a web search agent.
    Your only tool is search_tool - use it to find information.
    You make only one search call at a time.
    Once you have the results, you never do calculations based on them.

  
    
    
    "A data analyst agent. Useful for performing calculations."
    
    
    
    You are a data analyst.
    Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.

  
  # The termination condition is a combination of text mention termination and
max message termination.

    
    
      
  # The selector function is a function that takes the current message thread of
the group chat

  # and returns the next speaker's name. If None is returned, the LLM-based
selection method will be used.

          
       
        # Always return to the planning agent after the other agents have spoken.
     
    
      
     # Use a smaller model for the selector.
    
    
  
  
    
    
    "Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?"
  

```

Nested chat allows you to nest a whole team or another agent inside an agent.
This is useful for creating a hierarchical structure of agents or “information
silos”, as the nested agents cannot communicate directly with other agents
outside of the same group.

In , nested chat is supported by using the method on the class. You need to
specify the nested sequence of agents using dictionaries, See for more details.

In , nested chat is an implementation detail of a custom agent. You can create a
custom agent that takes a team or another agent as a parameter and implements
the method to trigger the nested team or agent. It is up to the application to
decide how to pass or transform the messages from and to the nested team or
agent.

The following example shows a simple nested chat that counts numbers.

```



  
  
  
  
    
  


"""An agent that returns a new number by adding 1 to the last number in the
input messages."""

          
       
         # Start from 0 if no messages are given.
    
        
         # Otherwise, start from the last number.
        
        
    
  
     
     


"""An agent that increments the last number in the input messages

  multiple times using a nested counting team."""

         
     "An agent that counts numbers."
      
          
    # Run the inner team with the given messages and returns the last message produced by the team.
        
    # To stream the inner messages, implement `on_messages_stream` and use that to implement `on_messages`.
      
      
        
    # Reset the inner team.
     
  
     
     
    
  # Create a team of two counting agents as the inner team.

     "An agent that counts numbers."
     "An agent that counts numbers."
      
  # Create a nested counting agent that takes the inner team as a parameter.

     
  # Run the nested counting agent with a message starting from 1.

       
      
     
    
  

```

You should see the following output:

You can take a look at for a more complex implementation.

In , sequential chat is supported by using the function. It takes input a list
of dictionary configurations for each step of the sequence. See for more
details.

Base on the feedback from the community, the function is too opinionated and not
flexible enough to support the diverse set of scenarios that users want to
implement. We often find users struggling to get the function to work when they
can easily glue the steps together usign basic Python code. Therefore, in , we
do not provide a built-in function for sequential chat in the AgentChat API.

Instead, you can create an event-driven sequential workflow using the Core API,
and use the other components provided the AgentChat API to implement each step
of the workflow. See an example of sequential workflow in the .

We recognize that the concept of workflow is at the heart of many applications,
and we will provide more built-in support for workflows in the future.

In , is a special agent class that is backed by the OpenAI Assistant API.

In , the equivalent is the class. It supports the same set of features as the in
with more such as customizable threads and file uploads. See for more details.

In , long context that overflows the model’s context window can be handled by
using the capability that is added to an after which is contructed.

The feedbacks from our community has led us to believe this feature is essential
and should be a built-in component of , and can be used for every custom agent.

In , we introduce the base class that manages message history and provides a
virtual view of the history. Applications can use built-in implementations such
as to limit the message history sent to the model, or provide their own
implementations that creates different virtual views.

To use in an in a chatbot scenario.

```



  
  
  
  
  
    
      
    
    
    "You are a helpful assistant."
    
     # Model can only view the last 10 messages.
  
  
      
       
      
         
     

```

In this example, the chatbot can only read the last 10 messages in the history.

In AgentChat, you can observe the agents by using the method which returns an
async generator to stream the inner thoughts and actions of the agent. For
teams, you can use the method to stream the inner conversation among the agents
in the team. Your application can use these streams to observe the agents and
teams in real-time.

Both the and methods takes a as a parameter which can be used to cancel the
output stream asynchronously and stop the agent or team. For teams, you can also
use termination conditions to stop the team when a certain condition is met. See
for more details.

Unlike the which comes with a special logging module, the API simply uses
Python’s module to log events such as model client calls. See in the Core API
documentation for more details.

The code executors in and are nearly identical except the executors support
async API. You can also use to cancel a code execution if it takes too long. See
[Command Line Code Executors
Tutorial](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-
guide/<../core-user-guide/framework/command-line-code-executors.html>) in the
Core API documentation.

We also added that can use Azure Container Apps (ACA) dynamic sessions for code
execution. See [ACA Dynamic Sessions Code Executor
Docs](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-
guide/<../extensions-user-guide/azure-container-code-executor.html>).

---

# Get Started
URL: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html

Via AgentChat, you can build applications quickly using preset agents. To
illustrate this, we will begin with creating a team of a single tool-use agent
that you can chat with.

The following code uses the OpenAI model. If you haven’t already, you need to
install the following package and extension:

To use Azure OpenAI models and AAD authentication, you can follow the
instructions .

```

  
  
  
  

     
    is 73 degrees and Sunny."

    
  
    
    
    
      
      
    
    
  
  # Define a team with a single agent and maximum auto-gen turns of 1.

     
  
    # Get user input from the console.
      "Enter a message (type 'exit' to leave): "
       
      
    # Run the team and stream messages to the console.
      
     

# NOTE: if running this inside a Python script you'll need to use
asyncio.run(main()).



```

```

---------- user ----------
What is the weather in NYC?

---------- weather_agent ----------
[FunctionCall(id='call_vN04UiNJgqSz6g3MHt7Renig', arguments='{"city":"New York
City"}', name='get_weather')]

[Prompt tokens: 75, Completion tokens: 16]

---------- weather_agent ----------
[FunctionExecutionResult(content='The weather in New York City is 73 degrees and
Sunny.', call_id='call_vN04UiNJgqSz6g3MHt7Renig')]

---------- weather_agent ----------
The weather in New York City is 73 degrees and Sunny.

---------- Summary ----------
Number of messages: 4

Finish reason: Maximum number of turns 1 reached.

Total prompt tokens: 75

Total completion tokens: 16

Duration: 1.15 seconds

---------- user ----------
What is the weather in Seattle?

---------- weather_agent ----------
[FunctionCall(id='call_BesYutZXJIMfu2TlDZgodIEj',
arguments='{"city":"Seattle"}', name='get_weather')]

[Prompt tokens: 127, Completion tokens: 14]

---------- weather_agent ----------
[FunctionExecutionResult(content='The weather in Seattle is 73 degrees and
Sunny.', call_id='call_BesYutZXJIMfu2TlDZgodIEj')]

---------- weather_agent ----------
The weather in Seattle is 73 degrees and Sunny.

---------- Summary ----------
Number of messages: 4

Finish reason: Maximum number of turns 1 reached.

Total prompt tokens: 127

Total completion tokens: 14

Duration: 2.38 seconds

```

The code snippet above introduces two high level concepts in AgentChat: and . An
Agent helps us define what actions are taken when a message is received.
Specifically, we use the preset - an agent that can be given access to a model
(e.g., LLM) and tools (functions) that it can then use to address tasks. A Team
helps us define the rules for how agents interact with each other. In the team,
agents respond in a sequential round-robin fashion. In this case, we have a
single agent, so the same agent is used for each round.

Now that you have a basic understanding of how to define an agent and a team,
consider following the for a walkthrough on other features of AgentChat.

---

# Get Started
URL: https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/quickstart.html

Before diving into the core APIs, let’s start with a simple example of two
agents that count down from 10 to 1.

We first define the agent classes and their respective procedures for handling
messages. We create two agent classes: and . The agent modifies a number that is
given and the agent checks the value against a condition. We also create a data
class, which defines the messages that are passed between the agents.

```

  
  
       



  



        
    
      
  
          
      
    
       



        
    
      
  
          
      
      
        
    
      

```

You might have already noticed, the agents’ logic, whether it is using model or
code executor, is completely decoupled from how messages are delivered. This is
the core idea: the framework provides a communication infrastructure, and the
agents are responsible for their own logic. We call the communication
infrastructure an .

Agent runtime is a key concept of this framework. Besides delivering messages,
it also manages agents’ lifecycle. So the creation of agents are handled by the
runtime.

The following code shows how to register and run the agents using , a local
embedded agent runtime implementation.

```

    
# Create an local embedded runtime.

  
# Register the modifier and checker agents by providing

# their agent types, the factory functions for creating instance and
subscriptions.



  
  
  # Modify the value by subtracting 1

       



  
  
  # Run until the value is less than or equal to 1

       

# Start the runtime and send a direct message to the checker.

  


```

```

--------------------------------------------------------------------------------
Checker:

10 passed the check, continue.

--------------------------------------------------------------------------------
Modifier:

Modified 10 to 9

--------------------------------------------------------------------------------
Checker:

9 passed the check, continue.

--------------------------------------------------------------------------------
Modifier:

Modified 9 to 8

--------------------------------------------------------------------------------
Checker:

8 passed the check, continue.

--------------------------------------------------------------------------------
Modifier:

Modified 8 to 7

--------------------------------------------------------------------------------
Checker:

7 passed the check, continue.

--------------------------------------------------------------------------------
Modifier:

Modified 7 to 6

--------------------------------------------------------------------------------
Checker:

6 passed the check, continue.

--------------------------------------------------------------------------------
Modifier:

Modified 6 to 5

--------------------------------------------------------------------------------
Checker:

5 passed the check, continue.

--------------------------------------------------------------------------------
Modifier:

Modified 5 to 4

--------------------------------------------------------------------------------
Checker:

4 passed the check, continue.

--------------------------------------------------------------------------------
Modifier:

Modified 4 to 3

--------------------------------------------------------------------------------
Checker:

3 passed the check, continue.

--------------------------------------------------------------------------------
Modifier:

Modified 3 to 2

--------------------------------------------------------------------------------
Checker:

2 passed the check, continue.

--------------------------------------------------------------------------------
Modifier:

Modified 2 to 1

--------------------------------------------------------------------------------
Checker:

1 failed the check, stopping.

```

From the agent’s output, we can see the value was successfully decremented from
10 to 1 as the modifier and checker conditions dictate.

AutoGen also supports a distributed agent runtime, which can host agents running
on different processes or machines, with different identities, languages and
dependencies.

To learn how to use agent runtime, communication, message handling, and
subscription, please continue reading the sections following this quick start.

---

# LangChainToolAdapter
URL: https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html#autogen_ext.tools.langchain.LangChainToolAdapter

Allows you to wrap a LangChain tool and make it available to AutoGen.

This class requires the extra for the package.

    
() – A LangChain tool to wrap

Use the from the package to create a tool that allows you to interact with a
Pandas DataFrame.

> ```


  
  
  
  
  
  
  
  

    
     
     
    
    
    
    
    
    "Use the `df` variable to access the dataset."
  
  
    
      "What's the average age of the passengers?"  
    
  

```

---

# OpenAIAssistantAgent
URL: https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html#autogen_ext.agents.openai.OpenAIAssistantAgent

An agent implementation that uses the OpenAI Assistant API to generate
responses.

This agent leverages the OpenAI Assistant API to create AI assistants with
capabilities like:

The agent maintains a thread of conversation and can use various tools including

  * Code interpreter: For executing code and working with files
  * File search: For searching through uploaded documents
  * Custom functions: For extending capabilities with user-defined tools

  * Supports multiple file formats including code, documents, images
  * Can handle up to 128 tools per assistant
  * Maintains conversation context in threads
  * Supports file uploads for code interpreter and search
  * Vector store integration for efficient file search
  * Automatic file parsing and embedding

You can use an existing thread or assistant by providing the or parameters.

```

  
  


  
  

  
    
  # Create an OpenAI client

     
  # Create an assistant with code interpreter

    
    
    
    
    
    "You are a helpful Python programming assistant."
    
  
  # Upload files for the assistant to use

    
  # Get response from the assistant

     
     "Analyze the data in data.csv" 
  
  
  
  
  

```

    
  * () – Name of the assistant
  * () – Description of the assistant’s purpose
  * () – OpenAI API client instance
  * () – Model to use (e.g. “gpt-4”)
  * () – System instructions for the assistant
  * () – Tools the assistant can use
  * () – ID of existing assistant to use
  * () – ID of existing thread to use
  * () – Additional metadata for the assistant
  * () – Response format settings
  * () – Temperature for response generation
  * () – Additional tool configuration
  * () – Top p sampling parameter

    
Delete the assistant if it was created by this instance.

    
Delete all files that were uploaded by this agent instance.

    
Delete the vector store if it was created by this instance.

    
Handle regular text messages by adding them to the thread.

    
Handle incoming messages and return a response.

    
Handle incoming messages and return a response.

    
Handle reset command by deleting new messages and runs since initialization.

    
Handle file uploads for the code interpreter.

    
Handle file uploads for file search.

    
The types of messages that the assistant agent produces.

---

# DockerCommandLineCodeExecutor
URL: https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor

Executes code through a command line environment in a Docker container.

This class requires the extra for the package:

The executor first saves each code block in a file in the working directory, and
then executes the code file in the container. The executor executes the code
blocks in the order they are received. Currently, the executor only supports
Python and shell scripts. For Python code, use the language “python” for the
code block. For shell scripts, use the language “bash”, “shell”, or “sh” for the
code block.

    
  * () – Docker image to use for code execution. Defaults to “python:3-slim”.
  * () – Name of the Docker container which is created. If None, will autogenerate a name. Defaults to None.
  * () – The timeout for code execution. Defaults to 60.
  * () – The working directory for the code execution. Defaults to Path(“.”).
  * () – The directory that will be bound
  * (_to the code executor container. Useful for cases where you want to_)
  * (_the container from within a container. Defaults to_)
  * () – If true, will automatically remove the Docker container when it is stopped. Defaults to True.
  * () – If true, will automatically stop the container when stop is called, when the context manager exits or when the Python process exits with atext. Defaults to True.
  * () – A list of functions that are available to the code executor. Default is an empty list.
  * () – The name of the module that will be created to store the functions. Defaults to “functions”.

    
(Experimental) The binding directory for the code execution container.

    
(Experimental) Execute the code blocks and return the result.

    
() – The code blocks to execute.

    
– The result of the code execution.

    
This method should be implemented by the code executor.

This method is called when the agent is reset.

    
(Experimental) Stop the code executor.

    
(Experimental) The timeout for code execution.

    
(Experimental) The working directory for the code execution.

---

# GrpcWorkerAgentRuntime
URL: https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime

An agent runtime for running remote or cross-language agents.

Agent messaging uses protobufs from and from .

Cross-language agents will additionally require all agents use shared protobuf
schemas for any message types that are sent between agents.

    
Add a new message serialization serializer to the runtime

Note: This will deduplicate serializers based on the type_name and
data_content_type properties

    
() – The serializer/s to add

    
Add a new subscription that the runtime should fulfill when processing published
messages

    
() – The subscription to add

    
Load the state of a single agent.

    
  * () – The agent id.
  * () – The saved state.

    
Get the metadata for an agent.

    
() – The agent id.

    
Save the state of a single agent.

The structure of the state is implementation defined and can be any JSON
serializable object.

    
() – The agent id.

    
– The saved state.

    
Load the state of the entire runtime, including all hosted agents. The state
should be the same as the one returned by .

    
() – The saved state.

    
Publish a message to all agents in the given namespace, or if no namespace is
provided, the namespace of the sender.

No responses are expected from publishing.

    
  * () – The message to publish.
  * () – The topic to publish the message to.
  * () – The agent which sent the message. Defaults to None.
  * () – Token used to cancel an in progress. Defaults to None.
  * () – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.

    
– If the message cannot be delivered.

    
Register an agent factory with the runtime associated with a specific type. The
type must be unique. This API does not add any subscriptions.

This is a low level API and usually the agent class’s method should be used
instead, as this also handles subscriptions automatically.

```

  
      
  



  



     
    
  
          
     

  
  

    
      
     



```

    
  * () – The type of agent this factory creates. It is not the same as agent class name. The parameter is used to differentiate between different factory functions rather than agent classes.
  * () – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use to access variables like the current runtime and agent ID.
  * () – The expected class of the agent, used for runtime validation of the factory. Defaults to None.

    
Remove a subscription from the runtime

    
() – id of the subscription to remove

    
– If the subscription does not exist

    
Save the state of the entire runtime, including all hosted agents. The only way
to restore the state is to pass it to .

The structure of the state is implementation defined and can be any JSON
serializable object.

    
– The saved state.

    
Send a message to an agent and get a response.

    
  * () – The message to send.
  * () – The agent to send the message to.
  * () – Agent which sent the message. Should be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
  * () – Token used to cancel an in progress . Defaults to None.

    
  * – If the recipient cannot handle the message.
  * – If the message cannot be delivered.
  * – Any other exception raised by the recipient.

    
– The response from the agent.

    
Start the runtime in a background task.

    
Stop the runtime when a signal is received.

    
Try to get the underlying agent instance by name and namespace. This is
generally discouraged (hence the long name), but can be useful in some cases.

If the underlying agent is not accessible, this will raise an exception.

    
  * () – The agent id.
  * () – The expected type of the agent. Defaults to Agent.

    
– The concrete agent instance.

    
  * – If the agent is not found.
  * – If the agent is not accessible, for example if it is located remotely.
  * – If the agent is not of the expected type.

    
    
Start the server in a background task.

    
Stop the server when a signal is received.

    
A gRPC servicer that hosts message delivery service for agents.

    
Missing associated documentation comment in .proto file.

    
Missing associated documentation comment in .proto file.

    
Missing associated documentation comment in .proto file.

---

# Discover Community Extensions
URL: https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/discover.html

Find samples, services and other things that work with AutoGen

Find AutoGen extensions for 3rd party tools, components and services

Find community samples and examples of how to use AutoGen

Model client for other LLMs like Gemini, etc. through the OpenAI API  
---  
Tool adapter for Model Context Protocol server tools

---

# Create New Extension
URL: https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/create-your-own.html

With the new package structure in 0.4, it is easier than ever to create and
publish your own extension to the AutoGen ecosystem. This page details some best
practices so that your extension package integrates well with the AutoGen
ecosystem.

There is no requirement about naming. But prefixing the package name with makes
it easier to find.

Whenever possible, extensions should implement the provided interfaces from the
package. This will allow for a more consistent experience for users.

To ensure that the extension works with the version of AutoGen that it was
designed for, it is recommended to specify the version of AutoGen the dependency
section of the with adequate constraints.

AutoGen embraces the use of type hints to provide a better development
experience. Extensions should use type hints whenever possible.

To make it easier for users to find your extension, sample, service or package,
you can to the GitHub repo.

More specific topics are also available:

In AutoGen 0.2 it was common to merge 3rd party extensions and examples into the
main repo. We are super appreciative of all of the users who have contributed to
the ecosystem notebooks, modules and pages in 0.2. However, in general we are
moving away from this model to allow for more flexibility and to reduce
maintenance burden.

There is the package for 1st party supported extensions, but we want to be
selective to manage maintenance load. If you would like to see if your extension
makes sense to add into , please open an issue and let’s discuss. Otherwise, we
encourage you to publish your extension as a separate package and follow the
guidance under to make it easy for users to find.

---

