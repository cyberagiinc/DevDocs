{
  "url": "https://microsoft.github.io/autogen/stable/",
  "content": "# Skip to main content\nURL: https://microsoft.github.io/autogen/stable/#main-content\n\n###  A framework for building AI agents and applications\n\nA console-based multi-agent assistant for web and file-based tasks. Built on\nAgentChat.\n\n```\n\npipinstall-Umagentic-one-cli\n\nm1\"Find flights from Seattle to Paris and format the result in a table\"\n\n```\n\nAn app for prototyping and managing agents without writing code. Built on\nAgentChat.\n\nA programming framework for building conversational single and multi-agent\napplications. Built on Core. Requires Python 3.10+.\n\n```\n\n# pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n\n\n\n  \n  \n    \n     \n  \n\n```\n\n_Start here if you are building conversational agents. ._\n\nAn event-driven programming framework for building scalable multi-agent AI\nsystems. Example scenarios:\n\n  * Deterministic and dynamic agentic workflows for business processes.\n  * Distributed agents for multi-language applications.\n\n_Start here if you are building workflows or distributed agent systems._\n\nImplementations of Core and AgentChat components that interface with external\nservices or other libraries. You can find and use community extensions or create\nyour own. Examples of built-in extensions:\n\n  * for running model-generated code in a Docker container.\n\n---\n\n# Switch to stable version\nURL: https://microsoft.github.io/autogen/stable/index.html\n\n###  A framework for building AI agents and applications\n\nA console-based multi-agent assistant for web and file-based tasks. Built on\nAgentChat.\n\n```\n\npipinstall-Umagentic-one-cli\n\nm1\"Find flights from Seattle to Paris and format the result in a table\"\n\n```\n\nAn app for prototyping and managing agents without writing code. Built on\nAgentChat.\n\nA programming framework for building conversational single and multi-agent\napplications. Built on Core. Requires Python 3.10+.\n\n```\n\n# pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n\n\n\n  \n  \n    \n     \n  \n\n```\n\n_Start here if you are building conversational agents. ._\n\nAn event-driven programming framework for building scalable multi-agent AI\nsystems. Example scenarios:\n\n  * Deterministic and dynamic agentic workflows for business processes.\n  * Distributed agents for multi-language applications.\n\n_Start here if you are building workflows or distributed agent systems._\n\nImplementations of Core and AgentChat components that interface with external\nservices or other libraries. You can find and use community extensions or create\nyour own. Examples of built-in extensions:\n\n  * for running model-generated code in a Docker container.\n\n---\n\n# Untitled Page\nURL: https://microsoft.github.io/autogen/stable/\n\n###  A framework for building AI agents and applications\n\nA console-based multi-agent assistant for web and file-based tasks. Built on\nAgentChat.\n\n```\n\npipinstall-Umagentic-one-cli\n\nm1\"Find flights from Seattle to Paris and format the result in a table\"\n\n```\n\nAn app for prototyping and managing agents without writing code. Built on\nAgentChat.\n\nA programming framework for building conversational single and multi-agent\napplications. Built on Core. Requires Python 3.10+.\n\n```\n\n# pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n\n\n\n  \n  \n    \n     \n  \n\n```\n\n_Start here if you are building conversational agents. ._\n\nAn event-driven programming framework for building scalable multi-agent AI\nsystems. Example scenarios:\n\n  * Deterministic and dynamic agentic workflows for business processes.\n  * Distributed agents for multi-language applications.\n\n_Start here if you are building workflows or distributed agent systems._\n\nImplementations of Core and AgentChat components that interface with external\nservices or other libraries. You can find and use community extensions or create\nyour own. Examples of built-in extensions:\n\n  * for running model-generated code in a Docker container.\n\n---\n\n# dev (main)\nURL: https://microsoft.github.io/autogen/dev/index.html\n\n###  A framework for building AI agents and applications\n\nA console-based multi-agent assistant for web and file-based tasks. Built on\nAgentChat.\n\n```\n\npipinstall-Umagentic-one-cli\n\nm1\"Find flights from Seattle to Paris and format the result in a table\"\n\n```\n\nAn app for prototyping and managing agents without writing code. Built on\nAgentChat.\n\nA programming framework for building conversational single and multi-agent\napplications. Built on Core. Requires Python 3.10+.\n\n```\n\n# pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n\n\n\n\n\n  \n     \n  \n\n```\n\n_Start here if you are building conversational agents. ._\n\nAn event-driven programming framework for building scalable multi-agent AI\nsystems. Example scenarios:\n\n  * Deterministic and dynamic agentic workflows for business processes.\n  * Distributed agents for multi-language applications.\n\n_Start here if you are building workflows or distributed agent systems._\n\nImplementations of Core and AgentChat components that interface with external\nservices or other libraries. You can find and use community extensions or create\nyour own. Examples of built-in extensions:\n\n  * for running model-generated code in a Docker container.\n\n---\n\n# 0.2\nURL: https://microsoft.github.io/autogen/0.2/index.html\n\nhas been released. Read the migration guide .\n\nAutoGen provides multi-agent conversation framework as a high-level abstraction.\nWith this framework, one can conveniently build LLM workflows.\n\nAutoGen offers a collection of working systems spanning a wide range of\napplications from various domains and complexities.\n\n### [Enhanced LLM Inference &\nOptimization](https://microsoft.github.io/autogen/0.2/</autogen/0.2/docs/Use-\nCases/enhanced_inference>)\n\nAutoGen supports enhanced LLM inference APIs, which can be used to improve\ninference performance and reduce cost.\n\nhas been released. Read the migration guide .\n\nAutoGen provides multi-agent conversation framework as a high-level abstraction.\nWith this framework, one can conveniently build LLM workflows.\n\nAutoGen offers a collection of working systems spanning a wide range of\napplications from various domains and complexities.\n\n### [Enhanced LLM Inference &\nOptimization](https://microsoft.github.io/autogen/0.2/</autogen/docs/Use-\nCases/enhanced_inference>)\n\nAutoGen supports enhanced LLM inference APIs, which can be used to improve\ninference performance and reduce cost.\n\n---\n\n# AgentChat\nURL: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html\n\nAgentChat is a high-level API for building multi-agent applications. It is built\non top of the package. For beginner users, AgentChat is the recommended starting\npoint. For advanced users, ’s event-driven programming model provides more\nflexibility and control over the underlying components.\n\nAgentChat provides intuitive defaults, such as with preset behaviors and with\npredefined .\n\nStep-by-step guide to using AgentChat, learn about agents, teams, and more\n\nMulti-agent coordination through a shared context and centralized, customizable\nselector\n\nMulti-agent coordination through a shared context and localized, tool-based\nselector\n\nSample code and use cases\n\nHow to migrate from AutoGen 0.2.x to 0.4.x.\n\n---\n\n# Core\nURL: https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html\n\nAutoGen core offers an easy way to quickly build event-driven, distributed,\nscalable, resilient AI agent systems. Agents are developed by using the . You\ncan build and run your agent system locally and easily move to a distributed\nsystem in the cloud when you are ready.\n\nKey features of AutoGen core include:\n\nAgents communicate through asynchronous messages, enabling event-driven and\nrequest/response communication models.\n\nEnable complex scenarios with networks of agents across organizational\nboundaries.\n\nPython & Dotnet interoperating agents today, with more languages coming soon.\n\nHighly customizable with features like custom agents, memory as a service, tools\nregistry, and model library.\n\nEasily trace and debug your agent systems.\n\nBuild event-driven, distributed, scalable, and resilient AI agent systems.\n\n---\n\n# Extensions\nURL: https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html\n\nAutoGen is designed to be extensible. The package contains many different\ncomponent implementations maintained by the AutoGen project. However, we\nstrongly encourage others to build their own components and publish them as part\nof the ecosytem.\n\nDiscover community extensions and samples\n\n---\n\n# Studio\nURL: https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html\n\nAutoGen Studio is a low-code interface built to help you rapidly prototype AI\nagents, enhance them with tools, compose them into teams and interact with them\nto accomplish tasks. It is built on - a high-level API for building multi-agent\napplications.\n\nCode for AutoGen Studio is on GitHub at\n\nAutoGen Studio is meant to help you rapidly prototype multi-agent workflows and\ndemonstrate an example of end user interfaces built with AutoGen. It is not\nmeant to be a production-ready app. Developers are encouraged to use the AutoGen\nframework to build their own applications, implementing authentication, security\nand other features required for deployed applications.\n\n## Capabilities - What Can You Do with AutoGen Studio?\n\nAutoGen Studio offers four main interfaces to help you build and manage multi-\nagent systems:\n\n  1.      * A visual interface for creating agent teams through declarative specification (JSON) or drag-and-drop\n     * Supports configuration of all core components: teams, agents, tools, models, and termination conditions\n     * Fully compatible with AgentChat’s component definitions\n  2.      * Interactive environment for testing and running agent teams\n     *        * Live message streaming between agents\n       * Visual representation of message flow through a control transition graph\n       * Interactive sessions with teams using UserProxyAgent\n       * Full run control with the ability to pause or stop execution\n  3.      * Central hub for discovering and importing community-created components\n     * Enables easy integration of third-party components\n  4.      * Export and run teams in python code\n     * Setup and test endpoints based on a team configuration\n     * Run teams in a docker container\n\nReview project roadmap and issues .\n\nWe welcome contributions to AutoGen Studio. We recommend the following general\nsteps to contribute to the project:\n\n  * Review the overall AutoGen project \n  * Please review the AutoGen Studio to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with \n  * Please use the tag tag for any issues, questions, and PRs related to Studio\n  * Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.\n  * Submit a pull request with your contribution!\n  * If you are modifying AutoGen Studio, it has its own devcontainer. See instructions in to use it\n\nAutoGen Studio is a research prototype and is **not meant to be used** in a\nproduction environment. Some baseline practices are encouraged e.g., using\nDocker code execution environment for your agents.\n\nHowever, other considerations such as rigorous tests related to jailbreaking,\nensuring LLMs only have access to the right keys of data given the end user’s\npermissions, and other security features are not implemented in AutoGen Studio.\n\nIf you are building a production application, please use the AutoGen framework\nand implement the necessary security features.\n\nAutoGen Studio is based on the project. It was adapted from a research prototype\nbuilt in October 2023 (original credits: Victor Dibia, Gagan Bansal, Adam\nFourney, Piali Choudhury, Saleema Amershi, Ahmed Awadallah, Chi Wang).\n\nIf you use AutoGen Studio in your research, please cite the following paper:\n\nTo begin, follow the to install AutoGen Studio.\n\n---\n\n# 0.2 Docs\nURL: https://microsoft.github.io/autogen/0.2/\n\nhas been released. Read the migration guide .\n\nAutoGen provides multi-agent conversation framework as a high-level abstraction.\nWith this framework, one can conveniently build LLM workflows.\n\nAutoGen offers a collection of working systems spanning a wide range of\napplications from various domains and complexities.\n\n### [Enhanced LLM Inference &\nOptimization](https://microsoft.github.io/autogen/0.2/</autogen/0.2/docs/Use-\nCases/enhanced_inference>)\n\nAutoGen supports enhanced LLM inference APIs, which can be used to improve\ninference performance and reduce cost.\n\n---\n\n# #\nURL: https://microsoft.github.io/autogen/stable/#autogen\n\n###  A framework for building AI agents and applications\n\nA console-based multi-agent assistant for web and file-based tasks. Built on\nAgentChat.\n\n```\n\npipinstall-Umagentic-one-cli\n\nm1\"Find flights from Seattle to Paris and format the result in a table\"\n\n```\n\nAn app for prototyping and managing agents without writing code. Built on\nAgentChat.\n\nA programming framework for building conversational single and multi-agent\napplications. Built on Core. Requires Python 3.10+.\n\n```\n\n# pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n\n\n\n  \n  \n    \n     \n  \n\n```\n\n_Start here if you are building conversational agents. ._\n\nAn event-driven programming framework for building scalable multi-agent AI\nsystems. Example scenarios:\n\n  * Deterministic and dynamic agentic workflows for business processes.\n  * Distributed agents for multi-language applications.\n\n_Start here if you are building workflows or distributed agent systems._\n\nImplementations of Core and AgentChat components that interface with external\nservices or other libraries. You can find and use community extensions or create\nyour own. Examples of built-in extensions:\n\n  * for running model-generated code in a Docker container.\n\n---\n\n# Get Started\nURL: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html\n\nis a generalist multi-agent system for solving open-ended web and file-based\ntasks across a variety of domains. It represents a significant step forward for\nmulti-agent systems, achieving competitive performance on a number of agentic\nbenchmarks (see the for full details).\n\nWhen originally released in Magentic-One was . We have now ported Magentic-One\nto use , providing a more modular and easier to use interface.\n\nTo this end, the Magentic-One orchestrator is now simply an AgentChat team,\nsupporting all standard AgentChat agents and features. Likewise, Magentic-One’s\n, , and agents are now broadly available as AgentChat agents, to be used in any\nAgentChat workflows.\n\nLastly, there is a helper class, , which bundles all of this together as it was\nin the paper with minimal configuration.\n\nFind additional information about Magentic-one in our and .\n\n: The figure above illustrates Magentic-One multi-agent team completing a\ncomplex task from the GAIA benchmark. Magentic-One’s Orchestrator agent creates\na plan, delegates tasks to other agents, and tracks progress towards the goal,\ndynamically revising the plan as needed. The Orchestrator can delegate tasks to\na FileSurfer agent to read and handle files, a WebSurfer agent to operate a web\nbrowser, or a Coder or Computer Terminal agent to write or execute code,\nrespectively.\n\nUsing Magentic-One involves interacting with a digital world designed for\nhumans, which carries inherent risks. To minimize these risks, consider the\nfollowing precautions:\n\n  1. : Run all tasks in docker containers to isolate the agents and prevent direct system attacks.\n  2. : Use a virtual environment to run the agents and prevent them from accessing sensitive data.\n  3. : Closely monitor logs during and after execution to detect and mitigate risky behavior.\n  4. : Run the examples with a human in the loop to supervise the agents and prevent unintended consequences.\n  5. : Restrict the agents’ access to the internet and other resources to prevent unauthorized actions.\n  6. : Ensure that the agents do not have access to sensitive data or resources that could be compromised. Do not share sensitive information with the agents. Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences. Moreover, be cautious that Magentic-One may be susceptible to prompt injection attacks from webpages.\n\n```\n\npipinstallautogen-agentchatautogen-extmagentic-one,openai\n\n# If using the MultimodalWebSurfer, you also need to install playwright\ndependencies:\n\nplaywrightinstall--with-depschromium\n\n```\n\nIf you haven’t done so already, go through the AgentChat tutorial to learn about\nthe concepts of AgentChat.\n\nThen, you can try swapping out a with . For example:\n\n```\n\n\n\n  \n  \n  \n  \n\n    \n    \n    \n    \n    \n  \n     \n   \"Provide a different proof for Fermat's Last Theorem\"\n\n```\n\nOr, use the Magentic-One agents in a team:\n\nThe example code may download files from the internet, execute code, and\ninteract with web pages. Ensure you are in a safe environment before running the\nexample code.\n\n```\n\n\n\n  \n  \n  \n  \n\n    \n    \n    \n    \n    \n  \n     \n   \"What is the UV index in Melbourne today?\"\n\n```\n\nMagentic-One work is based on a multi-agent architecture where a lead\nOrchestrator agent is responsible for high-level planning, directing other\nagents and tracking task progress. The Orchestrator begins by creating a plan to\ntackle the task, gathering needed facts and educated guesses in a Task Ledger\nthat is maintained. At each step of its plan, the Orchestrator creates a\nProgress Ledger where it self-reflects on task progress and checks whether the\ntask is completed. If the task is not yet completed, it assigns one of Magentic-\nOne other agents a subtask to complete. After the assigned agent completes its\nsubtask, the Orchestrator updates the Progress Ledger and continues in this way\nuntil the task is complete. If the Orchestrator finds that progress is not being\nmade for enough steps, it can update the Task Ledger and create a new plan. This\nis illustrated in the figure above; the Orchestrator work is thus divided into\nan outer loop where it updates the Task Ledger and an inner loop to update the\nProgress Ledger.\n\nOverall, Magentic-One consists of the following agents:\n\n  * Orchestrator: the lead agent responsible for task decomposition and planning, directing other agents in executing subtasks, tracking overall progress, and taking corrective actions as needed\n  * WebSurfer: This is an LLM-based agent that is proficient in commanding and managing the state of a Chromium-based web browser. With each incoming request, the WebSurfer performs an action on the browser then reports on the new state of the web page The action space of the WebSurfer includes navigation (e.g. visiting a URL, performing a web search); web page actions (e.g., clicking and typing); and reading actions (e.g., summarizing or answering questions). The WebSurfer relies on the accessibility tree of the browser and on set-of-marks prompting to perform its actions.\n  * FileSurfer: This is an LLM-based agent that commands a markdown-based file preview application to read local files of most types. The FileSurfer can also perform common navigation tasks such as listing the contents of directories and navigating a folder structure.\n  * Coder: This is an LLM-based agent specialized through its system prompt for writing code, analyzing information collected from the other agents, or creating new artifacts.\n  * ComputerTerminal: Finally, ComputerTerminal provides the team with access to a console shell where the Coder’s programs can be executed, and where new programming libraries can be installed.\n\nTogether, Magentic-One’s agents provide the Orchestrator with the tools and\ncapabilities that it needs to solve a broad variety of open-ended problems, as\nwell as the ability to autonomously adapt to, and act in, dynamic and ever-\nchanging web and file-system environments.\n\nWhile the default multimodal LLM we use for all agents is GPT-4o, Magentic-One\nis model agnostic and can incorporate heterogonous models to support different\ncapabilities or meet different cost requirements when getting tasks done. For\nexample, it can use different LLMs and SLMs and their specialized versions to\npower different agents. We recommend a strong reasoning model for the\nOrchestrator agent such as GPT-4o. In a different configuration of Magentic-One,\nwe also experiment with using OpenAI o1-preview for the outer loop of the\nOrchestrator and for the Coder, while other agents continue to use GPT-4o.\n\n---\n\n# Migrating from AutoGen 0.2?\nURL: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html\n\n# Migration Guide for v0.2 to v0.4\n\nThis is a migration guide for users of the versions of to the version, which\nintroduces a new set of APIs and features. The version contains breaking\nchanges. Please read this guide carefully. We still maintain the version in the\nbranch; however, we highly recommend you upgrade to the version.\n\nWe no longer have admin access to the PyPI package, and the releases from that\npackage are no longer from Microsoft since version 0.2.34. To continue use the\nversion of AutoGen, install it using . Please read our regarding forks.\n\nSince the release of AutoGen in 2023, we have intensively listened to our\ncommunity and users from small startups and large enterprises, gathering much\nfeedback. Based on that feedback, we built AutoGen , a from-the-ground-up\nrewrite adopting an asynchronous, event-driven architecture to address issues\nsuch as observability, flexibility, interactive control, and scale.\n\nThe API is layered: the is the foundation layer offering a scalable, event-\ndriven actor framework for creating agentic workflows; the is built on Core,\noffering a task-driven, high-level framework for building interactive agentic\napplications. It is a replacement for AutoGen .\n\nMost of this guide focuses on ’s AgentChat API; however, you can also build your\nown high-level framework using just the Core API.\n\nJump straight to the to get started with .\n\nWe provide a detailed guide on how to migrate your existing codebase from to .\n\nSee each feature below for detailed information on how to migrate.\n\n  * [Model Client for OpenAI-Compatible APIs](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#model-client-for-openai-compatible-apis>)\n  * [Conversable Agent and Register Reply](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#conversable-agent-and-register-reply>)\n  * [Save and Load Agent State](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#save-and-load-agent-state>)\n  * [Conversion between v0.2 and v0.4 Messages](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#conversion-between-v0-2-and-v0-4-messages>)\n  * [Save and Load Group Chat State](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#save-and-load-group-chat-state>)\n  * [Group Chat with Tool Use](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#group-chat-with-tool-use>)\n  * [Group Chat with Custom Selector (Stateflow)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/<#group-chat-with-custom-selector-stateflow>)\n\nThe following features currently in will be provided in the future releases of\nversions:\n\nWe will update this guide when the missing features become available.\n\nIn you configure the model client as follows, and create the object.\n\n> : In AutoGen 0.2, the OpenAI client would try configs in the list until one\n> worked. 0.4 instead expects a specfic model configuration to be chosen.\nIn , we offer two ways to create a model client.\n\nAutoGen 0.4 has a . Model clients are a great use case for this. See below for\nhow to create an OpenAI chat completion client.\n\n### Use model client class directly\n\n## Model Client for OpenAI-Compatible APIs\n\nYou can use a the to connect to an OpenAI-Compatible API, but you need to\nspecify the and .\n\n> : We don’t test all the OpenAI-Compatible APIs, and many of them works\n> differently from the OpenAI API even though they may claim to suppor it.\n> Please test them before using them.\nRead about in AgentChat Tutorial and more detailed information on\n\nSupport for other hosted models will be added in the future.\n\nIn , you create an assistant agent as follows:\n\n```\n\n  \n  \n      \n  \n  \n\n  \n  \n  \"You are a helpful assistant.\"\n\n  \n\n```\n\nIn , it is similar, but you need to specify instead of .\n\n```\n\n  \n  \n     \n  \n  \n  \"You are a helpful assistant.\"\n\n  \n\n```\n\nHowever, the usage is somewhat different. In , instead of calling , you call or\nto handle incoming messages. Furthermore, the and methods are asynchronous, and\nthe latter returns an async generator to stream the inner thoughts of the agent.\n\nHere is how you can call the assistant agent in directly, continuing from the\nabove example:\n\n```\n\n\n\n  \n  \n  \n  \n    \n      \n    \n    \n    \"You are a helpful assistant.\"\n    \n  \n    \n       \n  \n\n```\n\nThe can be used to cancel the request asynchronously when you call , which will\ncause the on the call to raise a .\n\nThe in supports multi-modal inputs if the model client supports it. The\ncapability of the model client is used to determine if the agent supports multi-\nmodal inputs.\n\n```\n\n\n\n  \n  \n  \n    \n  \n    \n      \n    \n    \n    \"You are a helpful assistant.\"\n    \n  \n    \n    \n     \n    \n  \n      \n  \n\n```\n\nIn , you create a user proxy as follows:\n\nThis user proxy would take input from the user through console, and would\nterminate if the incoming message ends with “TERMINATE”.\n\nIn , a user proxy is simply an agent that takes user input only, there is no\nother special configuration needed. You can create a user proxy as follows:\n\nSee for more details and how to customize the input function with timeout.\n\n## Conversable Agent and Register Reply\n\nIn , you can create a conversable agent and register a reply function as\nfollows:\n\n```\n\n        \n  \n  \n      \n  \n  \n\n  \n  \n  \"You are a helpful assistant.\"\n\n  \n  \n  \n  \n\n\n\n  \n     \n     \n     \n     \n  # Custom reply logic here\n\n    \n# Register the reply function\n\n  \n# NOTE: An async reply function will only be invoked with async send.\n\n```\n\nRather than guessing what the does, all its parameters, and what the should be,\nin , we can simply create a custom agent and implement the , , and methods.\n\nYou can then use the custom agent in the same way as the . See for more details.\n\n## Save and Load Agent State\n\nIn there is no built-in way to save and load an agent’s state: you need to\nimplement it yourself by exporting the attribute of and importing it back\nthrough the parameter.\n\nIn , you can call and methods on agents to save and load their state.\n\n```\n\n\n\n\n\n  \n  \n  \n  \n    \n      \n    \n    \n    \"You are a helpful assistant.\"\n    \n  \n    \n       \n  \n  \n     \n  # (Optional) Write state to disk.\n\n      \n     \n  # (Optional) Load it back from disk.\n\n      \n      \n     # Inspect the state, which contains the chat history.\n  # Carry on the chat.\n\n       \n  \n  # Load the state, resulting the agent to revert to the previous state before\nthe last message.\n\n  \n  # Carry on the same chat again.\n\n       \n\n```\n\nYou can also call and on any teams, such as to save and load the state of the\nentire team.\n\nIn , you can create a two-agent chat for code execution as follows:\n\n```\n\n  \n    \n  \n      \n  \n  \n\n  \n  \n  \"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE'\nif the task is done.\"\n\n  \n     \n\n  \n  \n  \n  \n  \n  \n     \n\n   \"Write a python script to print 'Hello, world!'\"\n\n# Intermediate messages are printed to the console directly.\n\n```\n\nTo get the same behavior in , you can use the and together in a .\n\n```\n\n\n\n    \n  \n    \n  \n  \n  \n    \n      \n    \n    \n    \"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\"\n    \n  \n    \n    \n    \n  \n  # The termination condition is a combination of text termination and max\nmessage termination, either of which will cause the chat to terminate.\n\n      \n  # The group chat will alternate between the assistant and the code executor.\n\n      \n  # `run_stream` returns an async generator to stream the intermediate messages.\n\n    \"Write a python script to print 'Hello, world!'\"\n  # `Console` is a simple UI to display the stream.\n\n  \n\n```\n\nIn , to create a tool use chatbot, you must have two agents, one for calling the\ntool and one for executing the tool. You need to initiate a two-agent chat for\nevery user request.\n\n```\n\n     \n  \n      \n  \n  \n\n  \n  \n  \"You are a helpful assistant. You can call tools to help user.\"\n\n  \n   # Set to 1 so that we return to the application after each assistant reply as\nwe are building a chatbot.\n\n  \n  \n  \n  \n  \n\n    \n    is 72 degree and sunny.\"\n# Register the tool function to the tool caller and executor.\n\n  \n\n\n    \n     \n    \n    \n    \n    \n     # To let the model reflect on the tool use, set to \"last_msg\" to return the tool call result directly.\n  \n  \n\n```\n\nIn , you really just need one agent – the – to handle both the tool calling and\ntool execution.\n\n```\n\n\n\n  \n  \n  \n  \n     # Async tool is possible too.\n    is 72 degree and sunny.\"\n    \n      \n    \n    \n    \"You are a helpful assistant. You can call tools to help user.\"\n    \n    \n     # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly.\n  \n  \n      \n       \n      \n         \n     \n\n```\n\nWhen using tool-equipped agents inside a group chat such as , you simply do the\nsame as above to add tools to the agents, and create a group chat with the\nagents.\n\nIn , you get a object from the method. For example:\n\n```\n\n  \n  \n  \n  \n\n # Get LLM-reflected summary of the chat.\n\n # Get the chat history.\n\n # Get the cost of the chat.\n\n # Get the human input solicited by the chat.\n\n```\n\nIn , you get a object from a or method. The object contains the which is the\nmessage history of the chat, including both agents’ private (tool calls, etc.)\nand public messages.\n\nThere are some notable differences between and :\n\n  * The list in uses different message format than the list.\n  * There is no field. It is up to the application to decide how to summarize the chat using the list.\n  * is not provided in the object, as the user input can be extracted from the list by filtering with the field.\n  * is not provided in the object, however, you can calculate the cost based on token usage. It would be a great community extension to add cost calculation. See .\n\n## Conversion between v0.2 and v0.4 Messages\n\nYou can use the following conversion functions to convert between a v0.4 message\nin and a v0.2 message in .\n\n```\n\n      \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n    \n  \n\n\n\n     \n     \n       \n  \n\"\"\"Convert a v0.4 AgentChat message to a v0.2 message.\n\n    message (AgentEvent | ChatMessage): The message to convert.\n    role (Literal[\"assistant\", \"user\", \"tool\"]): The role of the message.\n    image_detail (Literal[\"auto\", \"high\", \"low\"], optional): The detail level of image content in multi-modal message. Defaults to \"auto\".\n\n    Dict[str, Any]: The converted AutoGen v0.2 message.\n\n      \n          \n           \n    \n           \n       \n        \n           \n        \n        \n      \n         \n    \n             \n       \n      \n        \n           \n           \n              \n        \n      \n    \n        \n       \n      \n        \n           \n           \n           \n        \n      \n          \n           \n  \n     \n  \n\n       \n\"\"\"Convert a v0.2 message to a v0.4 AgentChat message.\"\"\"\n\n     \n       \n       \n      \n        \n          \n          \n          \n        \n      \n      \n     \n       \n       \n      \n        \n          \n          \n        \n      \n      \n    \n         \n        \n          \n         \n      \n         \n      \n    \n      \n  \n     \n\n```\n\nIn , you need to create a class and pass it into a , and have a participant that\nis a user proxy to initiate the chat. For a simple scenario of a writer and a\ncritic, you can do the following:\n\n```\n\n     \n  \n      \n  \n  \n\n  \n  \n  \n  \n  \n     \n\n  \n  \n  \n  \"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if\nthe task is done.\"\n\n  \n\n# Create a group chat with the writer and critic.\n\n     \n# Create a group chat manager to manage the group chat, use round-robin\nselection method.\n\n    \n# Initiate the chat with the editor, intermediate messages are printed to the\nconsole directly.\n\n  \n  \n  \"Write a short story about a robot that discovers it has feelings.\"\n\n```\n\nIn , you can use the to achieve the same behavior.\n\n```\n\n\n\n  \n  \n  \n  \n  \n    \n      \n    \n    \n    \n    \n    \n  \n    \n    \n    \n    \"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\"\n    \n  \n  # The termination condition is a text termination, which will cause the chat\nto terminate when the text \"APPROVE\" is received.\n\n    \n  # The group chat will alternate between the writer and the critic.\n\n       \n  # `run_stream` returns an async generator to stream the intermediate messages.\n\n    \"Write a short story about a robot that discovers it has feelings.\"\n  # `Console` is a simple UI to display the stream.\n\n  \n\n```\n\nFor LLM-based speaker selection, you can use the instead. See and for more\ndetails.\n\n> : In , you do not need to register functions on a user proxy to use tools in a\n> group chat. You can simply pass the tool functions to the as shown in the\n> section. The agent will automatically call the tools when needed. If your tool\n> doesn’t output well formed response, you can use the parameter to have the\n> model reflect on the tool use.\nIn , group chat with resume is a bit complicated. You need to explicitly save\nthe group chat messages and load them back when you want to resume the chat. See\n[Resuming Group Chat in v0.2](https://microsoft.github.io/autogen/stable/user-\nguide/agentchat-user-\nguide/<https:/microsoft.github.io/autogen/0.2/docs/topics/groupchat/resuming_groupchat>)\nfor more details.\n\nIn , you can simply call or again with the same group chat object to resume the\nchat. To export and load the state, you can use and methods.\n\n```\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n      \n    \n    \n    \n    \n    \n  \n    \n    \n    \n    \"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\"\n    \n  \n  # The termination condition is a text termination, which will cause the chat\nto terminate when the text \"APPROVE\" is received.\n\n    \n  # The group chat will alternate between the writer and the critic.\n\n      \n  \n\n    \n  \n    \n  # `run_stream` returns an async generator to stream the intermediate messages.\n\n    \"Write a short story about a robot that discovers it has feelings.\"\n  # `Console` is a simple UI to display the stream.\n\n  \n  # Save the state of the group chat and all participants.\n\n     \n      \n     \n  # Create a new team with the same participants configuration.\n\n    \n  # Load the state of the group chat and all participants.\n\n      \n      \n  \n  \n    \"Translate the story into Chinese.\"\n  \n\n```\n\n## Save and Load Group Chat State\n\nIn , you need to explicitly save the group chat messages and load them back when\nyou want to resume the chat.\n\nIn , you can simply call and methods on the group chat object. See for an\nexample.\n\n## Group Chat with Tool Use\n\nIn group chat, when tools are involved, you need to register the tool functions\non a user proxy, and include the user proxy in the group chat. The tool calls\nmade by other agents will be routed to the user proxy to execute.\n\nWe have observed numerous issues with this approach, such as the the tool call\nrouting not working as expected, and the tool call request and result cannot be\naccepted by models without support for function calling.\n\nIn , there is no need to register the tool functions on a user proxy, as the\ntools are directly executed within the , which publishes the response from the\ntool to the group chat. So the group chat manager does not need to be involved\nin routing tool calls.\n\nSee for an example of using tools in a group chat.\n\n## Group Chat with Custom Selector (Stateflow)\n\nIn group chat, when the is set to a custom function, it can override the default\nselection method. This is useful for implementing a state-based selection\nmethod. For more details, see [Custom Sepaker Selection in\nv0.2](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-\nguide/<https:/microsoft.github.io/autogen/0.2/docs/topics/groupchat/customized_speaker_selection>).\n\nIn , you can use the with to achieve the same behavior. The is a function that\ntakes the current message thread of the group chat and returns the next\nspeaker’s name. If is returned, the LLM-based selection method will be used.\n\nHere is an example of using the state-based selection method to implement a web\nsearch/analysis scenario.\n\n```\n\n\n\n  \n  \n    \n    \n  \n  \n  \n# Note: This example uses mock tools instead of real APIs for demonstration\npurposes\n\n    \n     \n     \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n\n     \n     \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n     \n     \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n  \n\n      \n         \n  \n    \n    \n    \n    \"An agent for planning tasks, this agent should be the first to engage when given a new task.\"\n    \n    \n    You are a planning agent.\n    Your job is to break down complex tasks into smaller, manageable subtasks.\n\n      Web search agent: Searches for information\n\n    You only plan and delegate tasks - you do not execute them yourself.\n    When assigning tasks, use this format:\n\n    After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n\n  \n    \n    \n    \n    \n    \n    \n    You are a web search agent.\n    Your only tool is search_tool - use it to find information.\n    You make only one search call at a time.\n    Once you have the results, you never do calculations based on them.\n\n  \n    \n    \n    \"A data analyst agent. Useful for performing calculations.\"\n    \n    \n    \n    You are a data analyst.\n    Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n\n  \n  # The termination condition is a combination of text mention termination and\nmax message termination.\n\n    \n    \n      \n  # The selector function is a function that takes the current message thread of\nthe group chat\n\n  # and returns the next speaker's name. If None is returned, the LLM-based\nselection method will be used.\n\n          \n       \n        # Always return to the planning agent after the other agents have spoken.\n     \n    \n      \n     # Use a smaller model for the selector.\n    \n    \n  \n  \n    \n    \n    \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n  \n\n```\n\nNested chat allows you to nest a whole team or another agent inside an agent.\nThis is useful for creating a hierarchical structure of agents or “information\nsilos”, as the nested agents cannot communicate directly with other agents\noutside of the same group.\n\nIn , nested chat is supported by using the method on the class. You need to\nspecify the nested sequence of agents using dictionaries, See for more details.\n\nIn , nested chat is an implementation detail of a custom agent. You can create a\ncustom agent that takes a team or another agent as a parameter and implements\nthe method to trigger the nested team or agent. It is up to the application to\ndecide how to pass or transform the messages from and to the nested team or\nagent.\n\nThe following example shows a simple nested chat that counts numbers.\n\n```\n\n\n\n  \n  \n  \n  \n    \n  \n\n\n\"\"\"An agent that returns a new number by adding 1 to the last number in the\ninput messages.\"\"\"\n\n          \n       \n         # Start from 0 if no messages are given.\n    \n        \n         # Otherwise, start from the last number.\n        \n        \n    \n  \n     \n     \n\n\n\"\"\"An agent that increments the last number in the input messages\n\n  multiple times using a nested counting team.\"\"\"\n\n         \n     \"An agent that counts numbers.\"\n      \n          \n    # Run the inner team with the given messages and returns the last message produced by the team.\n        \n    # To stream the inner messages, implement `on_messages_stream` and use that to implement `on_messages`.\n      \n      \n        \n    # Reset the inner team.\n     \n  \n     \n     \n    \n  # Create a team of two counting agents as the inner team.\n\n     \"An agent that counts numbers.\"\n     \"An agent that counts numbers.\"\n      \n  # Create a nested counting agent that takes the inner team as a parameter.\n\n     \n  # Run the nested counting agent with a message starting from 1.\n\n       \n      \n     \n    \n  \n\n```\n\nYou should see the following output:\n\nYou can take a look at for a more complex implementation.\n\nIn , sequential chat is supported by using the function. It takes input a list\nof dictionary configurations for each step of the sequence. See for more\ndetails.\n\nBase on the feedback from the community, the function is too opinionated and not\nflexible enough to support the diverse set of scenarios that users want to\nimplement. We often find users struggling to get the function to work when they\ncan easily glue the steps together usign basic Python code. Therefore, in , we\ndo not provide a built-in function for sequential chat in the AgentChat API.\n\nInstead, you can create an event-driven sequential workflow using the Core API,\nand use the other components provided the AgentChat API to implement each step\nof the workflow. See an example of sequential workflow in the .\n\nWe recognize that the concept of workflow is at the heart of many applications,\nand we will provide more built-in support for workflows in the future.\n\nIn , is a special agent class that is backed by the OpenAI Assistant API.\n\nIn , the equivalent is the class. It supports the same set of features as the in\nwith more such as customizable threads and file uploads. See for more details.\n\nIn , long context that overflows the model’s context window can be handled by\nusing the capability that is added to an after which is contructed.\n\nThe feedbacks from our community has led us to believe this feature is essential\nand should be a built-in component of , and can be used for every custom agent.\n\nIn , we introduce the base class that manages message history and provides a\nvirtual view of the history. Applications can use built-in implementations such\nas to limit the message history sent to the model, or provide their own\nimplementations that creates different virtual views.\n\nTo use in an in a chatbot scenario.\n\n```\n\n\n\n  \n  \n  \n  \n  \n    \n      \n    \n    \n    \"You are a helpful assistant.\"\n    \n     # Model can only view the last 10 messages.\n  \n  \n      \n       \n      \n         \n     \n\n```\n\nIn this example, the chatbot can only read the last 10 messages in the history.\n\nIn AgentChat, you can observe the agents by using the method which returns an\nasync generator to stream the inner thoughts and actions of the agent. For\nteams, you can use the method to stream the inner conversation among the agents\nin the team. Your application can use these streams to observe the agents and\nteams in real-time.\n\nBoth the and methods takes a as a parameter which can be used to cancel the\noutput stream asynchronously and stop the agent or team. For teams, you can also\nuse termination conditions to stop the team when a certain condition is met. See\nfor more details.\n\nUnlike the which comes with a special logging module, the API simply uses\nPython’s module to log events such as model client calls. See in the Core API\ndocumentation for more details.\n\nThe code executors in and are nearly identical except the executors support\nasync API. You can also use to cancel a code execution if it takes too long. See\n[Command Line Code Executors\nTutorial](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-\nguide/<../core-user-guide/framework/command-line-code-executors.html>) in the\nCore API documentation.\n\nWe also added that can use Azure Container Apps (ACA) dynamic sessions for code\nexecution. See [ACA Dynamic Sessions Code Executor\nDocs](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-\nguide/<../extensions-user-guide/azure-container-code-executor.html>).\n\n---\n\n# Get Started\nURL: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html\n\nVia AgentChat, you can build applications quickly using preset agents. To\nillustrate this, we will begin with creating a team of a single tool-use agent\nthat you can chat with.\n\nThe following code uses the OpenAI model. If you haven’t already, you need to\ninstall the following package and extension:\n\nTo use Azure OpenAI models and AAD authentication, you can follow the\ninstructions .\n\n```\n\n  \n  \n  \n  \n\n     \n    is 73 degrees and Sunny.\"\n\n    \n  \n    \n    \n    \n      \n      \n    \n    \n  \n  # Define a team with a single agent and maximum auto-gen turns of 1.\n\n     \n  \n    # Get user input from the console.\n      \"Enter a message (type 'exit' to leave): \"\n       \n      \n    # Run the team and stream messages to the console.\n      \n     \n\n# NOTE: if running this inside a Python script you'll need to use\nasyncio.run(main()).\n\n\n\n```\n\n```\n\n---------- user ----------\nWhat is the weather in NYC?\n\n---------- weather_agent ----------\n[FunctionCall(id='call_vN04UiNJgqSz6g3MHt7Renig', arguments='{\"city\":\"New York\nCity\"}', name='get_weather')]\n\n[Prompt tokens: 75, Completion tokens: 16]\n\n---------- weather_agent ----------\n[FunctionExecutionResult(content='The weather in New York City is 73 degrees and\nSunny.', call_id='call_vN04UiNJgqSz6g3MHt7Renig')]\n\n---------- weather_agent ----------\nThe weather in New York City is 73 degrees and Sunny.\n\n---------- Summary ----------\nNumber of messages: 4\n\nFinish reason: Maximum number of turns 1 reached.\n\nTotal prompt tokens: 75\n\nTotal completion tokens: 16\n\nDuration: 1.15 seconds\n\n---------- user ----------\nWhat is the weather in Seattle?\n\n---------- weather_agent ----------\n[FunctionCall(id='call_BesYutZXJIMfu2TlDZgodIEj',\narguments='{\"city\":\"Seattle\"}', name='get_weather')]\n\n[Prompt tokens: 127, Completion tokens: 14]\n\n---------- weather_agent ----------\n[FunctionExecutionResult(content='The weather in Seattle is 73 degrees and\nSunny.', call_id='call_BesYutZXJIMfu2TlDZgodIEj')]\n\n---------- weather_agent ----------\nThe weather in Seattle is 73 degrees and Sunny.\n\n---------- Summary ----------\nNumber of messages: 4\n\nFinish reason: Maximum number of turns 1 reached.\n\nTotal prompt tokens: 127\n\nTotal completion tokens: 14\n\nDuration: 2.38 seconds\n\n```\n\nThe code snippet above introduces two high level concepts in AgentChat: and . An\nAgent helps us define what actions are taken when a message is received.\nSpecifically, we use the preset - an agent that can be given access to a model\n(e.g., LLM) and tools (functions) that it can then use to address tasks. A Team\nhelps us define the rules for how agents interact with each other. In the team,\nagents respond in a sequential round-robin fashion. In this case, we have a\nsingle agent, so the same agent is used for each round.\n\nNow that you have a basic understanding of how to define an agent and a team,\nconsider following the for a walkthrough on other features of AgentChat.\n\n---\n\n# Get Started\nURL: https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/quickstart.html\n\nBefore diving into the core APIs, let’s start with a simple example of two\nagents that count down from 10 to 1.\n\nWe first define the agent classes and their respective procedures for handling\nmessages. We create two agent classes: and . The agent modifies a number that is\ngiven and the agent checks the value against a condition. We also create a data\nclass, which defines the messages that are passed between the agents.\n\n```\n\n  \n  \n       \n\n\n\n  \n\n\n\n        \n    \n      \n  \n          \n      \n    \n       \n\n\n\n        \n    \n      \n  \n          \n      \n      \n        \n    \n      \n\n```\n\nYou might have already noticed, the agents’ logic, whether it is using model or\ncode executor, is completely decoupled from how messages are delivered. This is\nthe core idea: the framework provides a communication infrastructure, and the\nagents are responsible for their own logic. We call the communication\ninfrastructure an .\n\nAgent runtime is a key concept of this framework. Besides delivering messages,\nit also manages agents’ lifecycle. So the creation of agents are handled by the\nruntime.\n\nThe following code shows how to register and run the agents using , a local\nembedded agent runtime implementation.\n\n```\n\n    \n# Create an local embedded runtime.\n\n  \n# Register the modifier and checker agents by providing\n\n# their agent types, the factory functions for creating instance and\nsubscriptions.\n\n\n\n  \n  \n  # Modify the value by subtracting 1\n\n       \n\n\n\n  \n  \n  # Run until the value is less than or equal to 1\n\n       \n\n# Start the runtime and send a direct message to the checker.\n\n  \n\n\n```\n\n```\n\n--------------------------------------------------------------------------------\nChecker:\n\n10 passed the check, continue.\n\n--------------------------------------------------------------------------------\nModifier:\n\nModified 10 to 9\n\n--------------------------------------------------------------------------------\nChecker:\n\n9 passed the check, continue.\n\n--------------------------------------------------------------------------------\nModifier:\n\nModified 9 to 8\n\n--------------------------------------------------------------------------------\nChecker:\n\n8 passed the check, continue.\n\n--------------------------------------------------------------------------------\nModifier:\n\nModified 8 to 7\n\n--------------------------------------------------------------------------------\nChecker:\n\n7 passed the check, continue.\n\n--------------------------------------------------------------------------------\nModifier:\n\nModified 7 to 6\n\n--------------------------------------------------------------------------------\nChecker:\n\n6 passed the check, continue.\n\n--------------------------------------------------------------------------------\nModifier:\n\nModified 6 to 5\n\n--------------------------------------------------------------------------------\nChecker:\n\n5 passed the check, continue.\n\n--------------------------------------------------------------------------------\nModifier:\n\nModified 5 to 4\n\n--------------------------------------------------------------------------------\nChecker:\n\n4 passed the check, continue.\n\n--------------------------------------------------------------------------------\nModifier:\n\nModified 4 to 3\n\n--------------------------------------------------------------------------------\nChecker:\n\n3 passed the check, continue.\n\n--------------------------------------------------------------------------------\nModifier:\n\nModified 3 to 2\n\n--------------------------------------------------------------------------------\nChecker:\n\n2 passed the check, continue.\n\n--------------------------------------------------------------------------------\nModifier:\n\nModified 2 to 1\n\n--------------------------------------------------------------------------------\nChecker:\n\n1 failed the check, stopping.\n\n```\n\nFrom the agent’s output, we can see the value was successfully decremented from\n10 to 1 as the modifier and checker conditions dictate.\n\nAutoGen also supports a distributed agent runtime, which can host agents running\non different processes or machines, with different identities, languages and\ndependencies.\n\nTo learn how to use agent runtime, communication, message handling, and\nsubscription, please continue reading the sections following this quick start.\n\n---\n\n# LangChainToolAdapter\nURL: https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html#autogen_ext.tools.langchain.LangChainToolAdapter\n\nAllows you to wrap a LangChain tool and make it available to AutoGen.\n\nThis class requires the extra for the package.\n\n    \n() – A LangChain tool to wrap\n\nUse the from the package to create a tool that allows you to interact with a\nPandas DataFrame.\n\n> ```\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n    \n     \n     \n    \n    \n    \n    \n    \n    \"Use the `df` variable to access the dataset.\"\n  \n  \n    \n      \"What's the average age of the passengers?\"  \n    \n  \n\n```\n\n---\n\n# OpenAIAssistantAgent\nURL: https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html#autogen_ext.agents.openai.OpenAIAssistantAgent\n\nAn agent implementation that uses the OpenAI Assistant API to generate\nresponses.\n\nThis agent leverages the OpenAI Assistant API to create AI assistants with\ncapabilities like:\n\nThe agent maintains a thread of conversation and can use various tools including\n\n  * Code interpreter: For executing code and working with files\n  * File search: For searching through uploaded documents\n  * Custom functions: For extending capabilities with user-defined tools\n\n  * Supports multiple file formats including code, documents, images\n  * Can handle up to 128 tools per assistant\n  * Maintains conversation context in threads\n  * Supports file uploads for code interpreter and search\n  * Vector store integration for efficient file search\n  * Automatic file parsing and embedding\n\nYou can use an existing thread or assistant by providing the or parameters.\n\n```\n\n  \n  \n\n\n  \n  \n\n  \n    \n  # Create an OpenAI client\n\n     \n  # Create an assistant with code interpreter\n\n    \n    \n    \n    \n    \n    \"You are a helpful Python programming assistant.\"\n    \n  \n  # Upload files for the assistant to use\n\n    \n  # Get response from the assistant\n\n     \n     \"Analyze the data in data.csv\" \n  \n  \n  \n  \n  \n\n```\n\n    \n  * () – Name of the assistant\n  * () – Description of the assistant’s purpose\n  * () – OpenAI API client instance\n  * () – Model to use (e.g. “gpt-4”)\n  * () – System instructions for the assistant\n  * () – Tools the assistant can use\n  * () – ID of existing assistant to use\n  * () – ID of existing thread to use\n  * () – Additional metadata for the assistant\n  * () – Response format settings\n  * () – Temperature for response generation\n  * () – Additional tool configuration\n  * () – Top p sampling parameter\n\n    \nDelete the assistant if it was created by this instance.\n\n    \nDelete all files that were uploaded by this agent instance.\n\n    \nDelete the vector store if it was created by this instance.\n\n    \nHandle regular text messages by adding them to the thread.\n\n    \nHandle incoming messages and return a response.\n\n    \nHandle incoming messages and return a response.\n\n    \nHandle reset command by deleting new messages and runs since initialization.\n\n    \nHandle file uploads for the code interpreter.\n\n    \nHandle file uploads for file search.\n\n    \nThe types of messages that the assistant agent produces.\n\n---\n\n# DockerCommandLineCodeExecutor\nURL: https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor\n\nExecutes code through a command line environment in a Docker container.\n\nThis class requires the extra for the package:\n\nThe executor first saves each code block in a file in the working directory, and\nthen executes the code file in the container. The executor executes the code\nblocks in the order they are received. Currently, the executor only supports\nPython and shell scripts. For Python code, use the language “python” for the\ncode block. For shell scripts, use the language “bash”, “shell”, or “sh” for the\ncode block.\n\n    \n  * () – Docker image to use for code execution. Defaults to “python:3-slim”.\n  * () – Name of the Docker container which is created. If None, will autogenerate a name. Defaults to None.\n  * () – The timeout for code execution. Defaults to 60.\n  * () – The working directory for the code execution. Defaults to Path(“.”).\n  * () – The directory that will be bound\n  * (_to the code executor container. Useful for cases where you want to_)\n  * (_the container from within a container. Defaults to_)\n  * () – If true, will automatically remove the Docker container when it is stopped. Defaults to True.\n  * () – If true, will automatically stop the container when stop is called, when the context manager exits or when the Python process exits with atext. Defaults to True.\n  * () – A list of functions that are available to the code executor. Default is an empty list.\n  * () – The name of the module that will be created to store the functions. Defaults to “functions”.\n\n    \n(Experimental) The binding directory for the code execution container.\n\n    \n(Experimental) Execute the code blocks and return the result.\n\n    \n() – The code blocks to execute.\n\n    \n– The result of the code execution.\n\n    \nThis method should be implemented by the code executor.\n\nThis method is called when the agent is reset.\n\n    \n(Experimental) Stop the code executor.\n\n    \n(Experimental) The timeout for code execution.\n\n    \n(Experimental) The working directory for the code execution.\n\n---\n\n# GrpcWorkerAgentRuntime\nURL: https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime\n\nAn agent runtime for running remote or cross-language agents.\n\nAgent messaging uses protobufs from and from .\n\nCross-language agents will additionally require all agents use shared protobuf\nschemas for any message types that are sent between agents.\n\n    \nAdd a new message serialization serializer to the runtime\n\nNote: This will deduplicate serializers based on the type_name and\ndata_content_type properties\n\n    \n() – The serializer/s to add\n\n    \nAdd a new subscription that the runtime should fulfill when processing published\nmessages\n\n    \n() – The subscription to add\n\n    \nLoad the state of a single agent.\n\n    \n  * () – The agent id.\n  * () – The saved state.\n\n    \nGet the metadata for an agent.\n\n    \n() – The agent id.\n\n    \nSave the state of a single agent.\n\nThe structure of the state is implementation defined and can be any JSON\nserializable object.\n\n    \n() – The agent id.\n\n    \n– The saved state.\n\n    \nLoad the state of the entire runtime, including all hosted agents. The state\nshould be the same as the one returned by .\n\n    \n() – The saved state.\n\n    \nPublish a message to all agents in the given namespace, or if no namespace is\nprovided, the namespace of the sender.\n\nNo responses are expected from publishing.\n\n    \n  * () – The message to publish.\n  * () – The topic to publish the message to.\n  * () – The agent which sent the message. Defaults to None.\n  * () – Token used to cancel an in progress. Defaults to None.\n  * () – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.\n\n    \n– If the message cannot be delivered.\n\n    \nRegister an agent factory with the runtime associated with a specific type. The\ntype must be unique. This API does not add any subscriptions.\n\nThis is a low level API and usually the agent class’s method should be used\ninstead, as this also handles subscriptions automatically.\n\n```\n\n  \n      \n  \n\n\n\n  \n\n\n\n     \n    \n  \n          \n     \n\n  \n  \n\n    \n      \n     \n\n\n\n```\n\n    \n  * () – The type of agent this factory creates. It is not the same as agent class name. The parameter is used to differentiate between different factory functions rather than agent classes.\n  * () – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use to access variables like the current runtime and agent ID.\n  * () – The expected class of the agent, used for runtime validation of the factory. Defaults to None.\n\n    \nRemove a subscription from the runtime\n\n    \n() – id of the subscription to remove\n\n    \n– If the subscription does not exist\n\n    \nSave the state of the entire runtime, including all hosted agents. The only way\nto restore the state is to pass it to .\n\nThe structure of the state is implementation defined and can be any JSON\nserializable object.\n\n    \n– The saved state.\n\n    \nSend a message to an agent and get a response.\n\n    \n  * () – The message to send.\n  * () – The agent to send the message to.\n  * () – Agent which sent the message. Should be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.\n  * () – Token used to cancel an in progress . Defaults to None.\n\n    \n  * – If the recipient cannot handle the message.\n  * – If the message cannot be delivered.\n  * – Any other exception raised by the recipient.\n\n    \n– The response from the agent.\n\n    \nStart the runtime in a background task.\n\n    \nStop the runtime when a signal is received.\n\n    \nTry to get the underlying agent instance by name and namespace. This is\ngenerally discouraged (hence the long name), but can be useful in some cases.\n\nIf the underlying agent is not accessible, this will raise an exception.\n\n    \n  * () – The agent id.\n  * () – The expected type of the agent. Defaults to Agent.\n\n    \n– The concrete agent instance.\n\n    \n  * – If the agent is not found.\n  * – If the agent is not accessible, for example if it is located remotely.\n  * – If the agent is not of the expected type.\n\n    \n    \nStart the server in a background task.\n\n    \nStop the server when a signal is received.\n\n    \nA gRPC servicer that hosts message delivery service for agents.\n\n    \nMissing associated documentation comment in .proto file.\n\n    \nMissing associated documentation comment in .proto file.\n\n    \nMissing associated documentation comment in .proto file.\n\n---\n\n# Discover Community Extensions\nURL: https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/discover.html\n\nFind samples, services and other things that work with AutoGen\n\nFind AutoGen extensions for 3rd party tools, components and services\n\nFind community samples and examples of how to use AutoGen\n\nModel client for other LLMs like Gemini, etc. through the OpenAI API  \n---  \nTool adapter for Model Context Protocol server tools\n\n---\n\n# Create New Extension\nURL: https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/create-your-own.html\n\nWith the new package structure in 0.4, it is easier than ever to create and\npublish your own extension to the AutoGen ecosystem. This page details some best\npractices so that your extension package integrates well with the AutoGen\necosystem.\n\nThere is no requirement about naming. But prefixing the package name with makes\nit easier to find.\n\nWhenever possible, extensions should implement the provided interfaces from the\npackage. This will allow for a more consistent experience for users.\n\nTo ensure that the extension works with the version of AutoGen that it was\ndesigned for, it is recommended to specify the version of AutoGen the dependency\nsection of the with adequate constraints.\n\nAutoGen embraces the use of type hints to provide a better development\nexperience. Extensions should use type hints whenever possible.\n\nTo make it easier for users to find your extension, sample, service or package,\nyou can to the GitHub repo.\n\nMore specific topics are also available:\n\nIn AutoGen 0.2 it was common to merge 3rd party extensions and examples into the\nmain repo. We are super appreciative of all of the users who have contributed to\nthe ecosystem notebooks, modules and pages in 0.2. However, in general we are\nmoving away from this model to allow for more flexibility and to reduce\nmaintenance burden.\n\nThere is the package for 1st party supported extensions, but we want to be\nselective to manage maintenance load. If you would like to see if your extension\nmakes sense to add into , please open an issue and let’s discuss. Otherwise, we\nencourage you to publish your extension as a separate package and follow the\nguidance under to make it easy for users to find.\n\n---\n\n",
  "timestamp": "2025-01-18T21:30:15.612Z",
  "stats": {
    "wordCount": 9031,
    "charCount": 64079
  }
}